{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate trips for full experiments\n",
    "\n",
    "Experiments:\n",
    "\n",
    "- Several iterations (tbc)\n",
    "- Temporal resolution (tbc)\n",
    "- Birmingham / Coventry\n",
    "- AM Peak / BH\n",
    "- Usual POI types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62304/1686199512.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import *\n",
    "from otp_routing_functions import *\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import multiprocessing\n",
    "import statistics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import HDBSCAN\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import yaml\n",
    "from shapely import Point\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "\n",
    "def init(trips, complete):\n",
    "    # Make num_trips global in each process.\n",
    "    # This grants read-only access in compute_trips\n",
    "    global num_trips\n",
    "    global rows_complete\n",
    "    num_trips = trips\n",
    "    rows_complete = complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Parameters\n",
    "decay_constant = config['decay_constant']\n",
    "exponent = config['exponent']\n",
    "#Integer for 1 trip per x minutes. E.g., 1 = a trip every minutes. 5 = 1 trip every 5 minutes\n",
    "# For now go with every minutes but may pull this back\n",
    "# To do - revisit later on\n",
    "temp_resolution = config['temp_resolution']\n",
    "num_iterations = config['num_iterations']\n",
    "num_oas = config['num_oas']\n",
    "num_pois = config['num_pois']\n",
    "processes = config['processes']\n",
    "otps = config['otps']\n",
    "host = config['host']\n",
    "port = config['port']\n",
    "\n",
    "# Vectrise distance decay function\n",
    "vfunc = np.vectorize(distance_decay)\n",
    "\n",
    "stratumDict = {\n",
    "    'wdam':{\n",
    "        'startHour' : 6,\n",
    "        'startMinute' : 30,\n",
    "        'endHour' : 8,\n",
    "        'endMinute' : 30\n",
    "        },\n",
    "    'wdpm':{\n",
    "        'startHour' : 16,\n",
    "        'startMinute' : 00,\n",
    "        'endHour' : 18,\n",
    "        'endMinute' : 30\n",
    "        },\n",
    "    'sat':{\n",
    "        'startHour' : 10,\n",
    "        'startMinute' : 00,\n",
    "        'endHour' : 16,\n",
    "        'endMinute' : 00\n",
    "        },\n",
    "    'bh':{\n",
    "        'startHour' : 10,\n",
    "        'startMinute' : 00,\n",
    "        'endHour' : 16,\n",
    "        'endMinute' : 00\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get day index\n",
    "# Specify the start and end dates\n",
    "start_date = '2024-03-15'\n",
    "end_date = '2024-04-15'\n",
    "\n",
    "# Create a date-time index\n",
    "date_index = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "experiment_dates = pd.DataFrame(index = date_index)\n",
    "experiment_dates['weekday'] = experiment_dates.index.weekday < 5\n",
    "experiment_dates['saturday'] = experiment_dates.index.weekday == 5\n",
    "bank_holidays = ['2024-03-29', '2024-04-01']\n",
    "experiment_dates['bank_holiday'] = experiment_dates.index.isin(pd.to_datetime(bank_holidays))\n",
    "\n",
    "# Get OAs\n",
    "wm_oas = gpd.read_file('data/west_midlands_OAs/west_midlands_OAs.shp')\n",
    "oa_info = pd.read_csv('data/oa_info.csv')\n",
    "oa_info = oa_info.merge(wm_oas[['OA11CD']], left_on = 'oa_id', right_on = 'OA11CD', how = 'inner')\n",
    "oaLatLon = oa_info[['oa_id','oa_lon','oa_lat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_meta_data = []\n",
    "for area_lad in ['E08000025','E08000026']:\n",
    "\n",
    "    if area_lad == 'E08000025':\n",
    "        pois = pd.read_csv('data/POIs/pois_birm.csv', index_col=0)\n",
    "    else:\n",
    "        pois = pd.read_csv('data/POIs/pois_cov.csv', index_col=0)\n",
    "    trip_oas = oa_info[oa_info['oa_id'].isin(list(wm_oas[wm_oas['LAD11CD'] == area_lad]['OA11CD']))]\n",
    "\n",
    "    for p_type in ['School','GP Surgery','Vaccination Centre','Hospital','Job Centre']:\n",
    "        trip_pois = pois[pois['type'] == p_type]\n",
    "        for stratum in ['wdam','wdpm','sat','bh']:\n",
    "            num_trips_odt = 0\n",
    "            num_trips_gtgm = 0\n",
    "            trip_generation_cost = 0\n",
    "            next_exp_meta = {}\n",
    "            next_exp_meta['area'] = area_lad\n",
    "            next_exp_meta['poi_type'] = p_type\n",
    "            next_exp_meta['poi_type'] = stratum\n",
    "\n",
    "            for it in range(5):\n",
    "\n",
    "                print()\n",
    "                print()\n",
    "                print('---- NEXT IT -----')\n",
    "                print('Area : {}'.format(area_lad))\n",
    "                print('POI Type : {}'.format(p_type))\n",
    "                print('Stratum : {}'.format(stratum))\n",
    "                print('Iteration : {}'.format(it))\n",
    "\n",
    "                t0 = time.time()\n",
    "\n",
    "                if stratum == 'wdam' or stratum == 'wdpm' :\n",
    "                    study_date = experiment_dates[(experiment_dates['weekday']) & (experiment_dates['bank_holiday'] == False)].sample(1).index\n",
    "\n",
    "                elif stratum == 'sat':\n",
    "                    study_date = experiment_dates[(experiment_dates['saturday']) & (experiment_dates['bank_holiday'] == False)].sample(1).index\n",
    "\n",
    "                elif stratum == 'bh':\n",
    "                    study_date = experiment_dates[experiment_dates['bank_holiday']].sample(1).index\n",
    "\n",
    "                # Create Time Domain\n",
    "                startHour = stratumDict[stratum]['startHour']\n",
    "                startMinute = stratumDict[stratum]['startMinute']\n",
    "                endHour = stratumDict[stratum]['endHour']\n",
    "                endMinute = stratumDict[stratum]['endMinute']\n",
    "\n",
    "                start = datetime(year=2012, month=2, day=25, hour=startHour, minute = startMinute)\n",
    "                end = datetime(year=2012, month=2, day=25, hour=endHour, minute = endMinute)\n",
    "                diff = end - start\n",
    "                minutesInInterval = diff.total_seconds()/60\n",
    "                hoursInInterval = minutesInInterval/60\n",
    "\n",
    "                num_trips = int((60 / temp_resolution) * hoursInInterval)\n",
    "\n",
    "                timeDomain = []\n",
    "\n",
    "                for i in range(num_trips):\n",
    "                    randStartTime = start + timedelta(minutes=random.randint(1, int(minutesInInterval)))\n",
    "                    if randStartTime not in timeDomain:\n",
    "                        timeDomain.append(str(randStartTime.hour).zfill(2)+':'+str(randStartTime.minute).zfill(2))\n",
    "\n",
    "                # Gravit Trip Generator\n",
    "                distMxList = []\n",
    "\n",
    "                for i,r in trip_oas.iterrows():\n",
    "                    for i_, r_ in trip_pois.iterrows():\n",
    "                        rowAppend = {}\n",
    "                        rowAppend['oa'] = r['oa_id']\n",
    "                        rowAppend['oa_lat'] = r['oa_lat']\n",
    "                        rowAppend['oa_lon'] = r['oa_lon']\n",
    "                        rowAppend['poi'] = r_['poi_id']\n",
    "                        rowAppend['poi_lat'] = r_['poi_lat']\n",
    "                        rowAppend['poi_lon'] = r_['poi_lon']\n",
    "                        rowAppend['dist'] = haversine_distance(r['oa_lon'], r['oa_lat'], r_['poi_lon'], r_['poi_lat'])\n",
    "                        distMxList.append(rowAppend)\n",
    "\n",
    "                distMx = pd.DataFrame(distMxList)\n",
    "                distMx['att'] = 1\n",
    "\n",
    "                distsDecay = []\n",
    "                for i in np.array(distMx['dist']):\n",
    "                    distsDecay.append(distance_decay(i, decay_constant, exponent))\n",
    "\n",
    "                distMx['decay'] = distsDecay\n",
    "                distMx['grav'] = distMx['decay'] * distMx['att']\n",
    "                distMx['gravN'] = (distMx['grav'] - distMx['grav'].min()) / (distMx['grav'].max() - distMx['grav'].min())\n",
    "                distMx['num_trips'] = (distMx['gravN'] * len(timeDomain)).astype(int)\n",
    "\n",
    "                num_trips_gtgm += (distMx['num_trips'].sum())\n",
    "                num_trips_odt += (len(distMx) * len(timeDomain))\n",
    "\n",
    "                # Generate\n",
    "                # Output trips to CSV\n",
    "                temp_trips_file = 'tempdata/trips_to_route_{}_{}_{}.csv'.format(area_lad,p_type,stratum)\n",
    "                output_file = open(temp_trips_file, 'w')\n",
    "                writer = csv.writer(output_file)\n",
    "                writer.writerow(['oa_id','poi_id','trip_id','date','time','oa_lat','oa_lon','poi_lat','poi_lon'])\n",
    "\n",
    "                #Output trips dataset - output csv with following: \n",
    "                trip_id = 0\n",
    "                #trip_date = study_date[0].strftime('%m/%d/%Y')\n",
    "                trip_date = study_date[0].strftime('%Y-%m-%d')\n",
    "\n",
    "                for i,r in distMx.iterrows():\n",
    "                    sample_trip_time = random.sample(timeDomain, r['num_trips'])\n",
    "                    for t in sample_trip_time:\n",
    "                        row = [r['oa'],r['poi'],trip_id,trip_date,t,r['oa_lat'], r['oa_lon'],r['poi_lat'], r['poi_lon']]\n",
    "                        writer.writerow(row)\n",
    "                        trip_id += 1\n",
    "\n",
    "                t1 = time.time()\n",
    "\n",
    "                trip_generation_cost += (t1 - t0)\n",
    "\n",
    "            t0 = time.time()\n",
    "            # Cost trips on OTP using parallelisation\n",
    "            num_trips = num_rows(temp_trips_file)\n",
    "            step_size = get_step_size(num_trips, processes)\n",
    "\n",
    "            args = []\n",
    "            for i in range(processes):\n",
    "                host_url = f\"http://{host}:{str(port + (i % otps))}\"\n",
    "                offset =i * step_size\n",
    "                arg = (\n",
    "                    host_url, \n",
    "                    offset, \n",
    "                    min(offset+step_size, num_trips), \n",
    "                    temp_trips_file, \n",
    "                    'tempdata'\n",
    "                )\n",
    "                args.append(arg)\n",
    "\n",
    "            rows_complete = multiprocessing.Value('i', 0)\n",
    "\n",
    "            t0_routing_cost = time.time()\n",
    "            with multiprocessing.Pool(int(processes), initializer=init, initargs=(num_trips, rows_complete)) as pool:\n",
    "                results = pool.starmap(compute_trips, args)\n",
    "            t1_routing_cost = time.time()\n",
    "\n",
    "            files = []\n",
    "            bad_rows = 0\n",
    "            for f, rows in results:\n",
    "                files.append(f)\n",
    "                bad_rows += rows\n",
    "            if bad_rows > 0:\n",
    "                print(f\"{bad_rows} trips were lost during OTP processing ({(bad_rows/num_trips * 100):.2f}%).\")\n",
    "\n",
    "            output_file_name = os.path.join('tempdata', 'results_full_{}_{}_{}.csv'.format(area_lad,p_type,stratum))\n",
    "            files_iterator = iter(files)\n",
    "            # Treat the first file differently, so we can extract headers\n",
    "            first_file = files[0]\n",
    "            headers = extract_headers(first_file)\n",
    "            with open(output_file_name, 'w') as output_file:\n",
    "                output_csv = csv.writer(output_file)\n",
    "                output_csv.writerow(headers)\n",
    "                for csv_file in files:\n",
    "                    with open(csv_file, newline='') as f:\n",
    "                        reader = csv.DictReader(f)\n",
    "                        for line in reader:\n",
    "                            output_csv.writerow(line.values())\n",
    "                        \n",
    "                        \n",
    "            #Delete all file in tempdata\n",
    "            files = glob.glob('tempdata/temp_*')\n",
    "            for f in files:\n",
    "                os.remove(f)\n",
    "            t1 = time.time()\n",
    "            \n",
    "            next_exp_meta['odt_trips'] = num_trips_odt\n",
    "            next_exp_meta['gtgm_trips'] = num_trips_gtgm\n",
    "            next_exp_meta['time_cost'] = (t1-t0)\n",
    "            next_exp_meta['bad rows'] = bad_rows\n",
    "            experiment_meta_data.append(next_exp_meta)\n",
    "\n",
    "        exp_meta_data_df = pd.DataFrame(experiment_meta_data)\n",
    "        exp_meta_data_df.to_csv('ostaat_trips_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "access",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
