{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import multiprocessing\n",
    "import os\n",
    "from typing import Tuple\n",
    "import time\n",
    "import itertools\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def num_rows(file_name: str) -> int:\n",
    "    '''Count the number of rows in the (CSV) file'''\n",
    "    with open(file_name, 'r') as otp_trips:\n",
    "        otp_trips_reader = csv.reader(otp_trips)\n",
    "        rows = sum(1 for row in otp_trips_reader)\n",
    "    rows -= 1 # Account for the header\n",
    "    return rows\n",
    "\n",
    "def get_step_size(num_trips: int, processes: int) -> int:\n",
    "    '''Calculate how many rows each process will have'''\n",
    "    return int(np.ceil(num_trips / processes))\n",
    "\n",
    "def init(trips, complete):\n",
    "    # Make num_trips global in each process.\n",
    "    # This grants read-only access in compute_trips\n",
    "    global num_trips\n",
    "    global rows_complete\n",
    "    num_trips = trips\n",
    "    rows_complete = complete\n",
    "\n",
    "def get_csv_section(reader, offset, limit) -> object:\n",
    "    '''Get a 'slice' or section of the CSV file for a process'''\n",
    "    return itertools.islice(reader, offset, limit)\n",
    "\n",
    "def request_otp(host_url, input_row):\n",
    "    url = host_url + '/otp/routers/default/plan?'\n",
    "    params = {\n",
    "        \"fromPlace\": f\"{input_row['oa_lat']},{input_row['oa_lon']}\",\n",
    "        \"toPlace\": f\"{input_row['poi_lat']},{input_row['poi_lon']}\",\n",
    "        \"date\": f\"{input_row['date']}\",\n",
    "        \"time\": f\"{input_row['time']}\",\n",
    "        \"mode\": \"TRANSIT,WALK\",\n",
    "        \"arriveBy\": \"false\",\n",
    "        \"numItineraries\": \"1\",\n",
    "        #\"maxWalkDistance\": \"1000\"\n",
    "        \"walkReluctance\": \"20\"\n",
    "    }\n",
    "    resp = requests.get(\n",
    "        url=url,\n",
    "        params=params,\n",
    "        headers={'accept': 'application/xml'}\n",
    "    )\n",
    "    return resp\n",
    "\n",
    "def get_request_parameter(node: ET.Element, param: str) -> str:\n",
    "    request_parameters = node.find('requestParameters')\n",
    "    return request_parameters.find(param).text\n",
    "\n",
    "def get_time_from_itinerary(time: str, itinerary):\n",
    "    dt = datetime.fromtimestamp(float(itinerary.find(time).text) / 1000)\n",
    "    dt += timedelta(hours=1)\n",
    "    return dt\n",
    "\n",
    "def get_total_distance_from_itinerary(itinerary):\n",
    "    total_dist = 0.0\n",
    "    for legs in itinerary.findall('legs'):\n",
    "        if legs.find('legs') is not None:\n",
    "            for leg in legs.findall('legs'):\n",
    "                total_dist += float(leg.find('distance').text)\n",
    "    return total_dist\n",
    "\n",
    "def get_fare_from_itinerary(itinerary):\n",
    "    if itinerary.find('fare') is not None:\n",
    "        fare_obj = itinerary.find('fare')\n",
    "        if fare_obj.find('details') is not None:\n",
    "            return float(fare_obj.find('details').find('regular').find('price').find('cents').text) / 100\n",
    "\n",
    "\n",
    "def calculate_fare(num_transfers, walk_time, total_time):\n",
    "    if walk_time == total_time:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 2.40 * (num_transfers + 1)\n",
    "\n",
    "\n",
    "def validate_trip(trip: dict) -> bool:\n",
    "    for value in trip.values():\n",
    "        if value is None:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def parse_response(response):\n",
    "    root = ET.fromstring(response.content)\n",
    "    trip = {\n",
    "        'departure_time': None,\n",
    "        'arrival_time': None,\n",
    "        'total_time': None,\n",
    "        'walk_time': None,\n",
    "        'transfer_wait_time': None,\n",
    "        'transit_time': None,\n",
    "        'walk_dist': None,\n",
    "        'transit_dist': None,\n",
    "        'total_dist': None,\n",
    "        'num_transfers': None,\n",
    "        'initial_wait_time': None,\n",
    "        'fare': None\n",
    "    }\n",
    "    date = get_request_parameter(root, 'date')\n",
    "    time = get_request_parameter(root, 'time')\n",
    "    query_time = datetime.strptime(' '.join([date, time]), '%Y-%m-%d %H:%M')\n",
    "    # Check if there was an error in the OTP response\n",
    "    trip_valid = False\n",
    "    if root.find('error').find('msg') is not None:\n",
    "        # The start and destination were too close, no trip could be found\n",
    "        if root.find('error').find('message').text in \"TOO_CLOSE\":\n",
    "            trip['departure_time'] = query_time\n",
    "            trip['arrival_time'] = query_time\n",
    "            trip['total_time'] = 0.0\n",
    "            trip['walk_time'] = 0.0\n",
    "            trip['transfer_wait_time'] = 0.0\n",
    "            trip['transit_time'] = 0.0\n",
    "            trip['walk_dist'] = 0.0\n",
    "            trip['transit_dist'] = 0.0\n",
    "            trip['total_dist'] = 0.0\n",
    "            trip['num_transfers'] = 0\n",
    "            trip['initial_wait_time'] = 0.0\n",
    "            trip['fare'] = 0.0\n",
    "            trip_valid = True\n",
    "    else:\n",
    "        plan = root.find('plan')\n",
    "        # Go through the iteneraries found in the plan. Should only be 1\n",
    "        for itineraries in plan.findall('itineraries'):\n",
    "            if itineraries.find('itineraries') is not None:  # Note that this line discards error XML, where there was no route\n",
    "                for itinerary in itineraries.findall('itineraries'):\n",
    "                    format_str = '%Y-%m-%d %H:%M:%S'\n",
    "                    trip['arrival_time'] = get_time_from_itinerary('endTime', itinerary).strftime(format_str)\n",
    "                    trip['total_time'] = float(itinerary.find('duration').text)\n",
    "                    trip['walk_time'] = float(itinerary.find('walkTime').text)\n",
    "                    trip['transfer_wait_time'] = float(itinerary.find('waitingTime').text)\n",
    "                    trip['transit_time'] = float(itinerary.find('transitTime').text)\n",
    "                    trip['walk_dist'] = float(itinerary.find('walkDistance').text)\n",
    "                    trip['num_transfers'] = int(itinerary.find('transfers').text)\n",
    "                    trip['total_dist'] = get_total_distance_from_itinerary(itinerary)\n",
    "                    trip['fare'] = get_fare_from_itinerary(itinerary)\n",
    "                    if trip['fare'] is None:\n",
    "                        trip['fare'] = calculate_fare(trip['num_transfers'], trip['walk_time'], trip['total_time'])\n",
    "                    # capture the wait time before the first bus arrives\n",
    "                    departure_time = get_time_from_itinerary('startTime', itinerary)\n",
    "                    trip['initial_wait_time'] = (departure_time - query_time).total_seconds()\n",
    "                    trip['departure_time'] = departure_time.strftime(format_str)\n",
    "                    trip['transit_dist'] = trip['total_dist'] - trip['walk_dist']\n",
    "                    trip_valid = validate_trip(trip)\n",
    "    if trip_valid:\n",
    "        return trip\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def compute_trips(host_url: str, offset: int, limit: int, input_file: str, output_dir: str) -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Send a request to OTP, parse the response and write a line to the output file.\n",
    "    Note: Parallel processing begins and ends here - each Python process will run this\n",
    "    function until it has completed its 'chunk' of data, then it will return the name\n",
    "    of the file it wrote its data to.\n",
    "    Recall that individual processes do not share global variables and other data - each \n",
    "    process holds a copy of the parent's (process it was created from) data independently \n",
    "    of any other process.\n",
    "    \"\"\"\n",
    "    process_id = os.getpid()\n",
    "    output_file = os.path.join(output_dir, f'temp_{process_id}.csv')\n",
    "    bad_rows = 0\n",
    "    with open(input_file, 'r') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        csv_section = get_csv_section(reader, offset, limit)\n",
    "        firstRow = next(csv_section)\n",
    "        t0 = time.time()\n",
    "        first_response = get_otp_response(host_url, firstRow)\n",
    "        t1 = time.time()\n",
    "        first_response['queryTime'] = t1 - t0\n",
    "        headers = first_response.keys()\n",
    "        \n",
    "        with open(output_file, 'a', newline='') as output_csv:\n",
    "            writer = csv.DictWriter(output_csv, fieldnames=headers, delimiter=',')\n",
    "            writer.writeheader()\n",
    "            writer.writerow(first_response)\n",
    "            row_counter = 1\n",
    "            for row in csv_section:\n",
    "                t0 = time.time()\n",
    "                response = get_otp_response(host_url, row)\n",
    "                t1 = time.time()\n",
    "                # Some trips have None for all attributes due to OTP error or inability to find a trip\n",
    "                # These trips return 'False' instead of a dict so empty rows are not written to CSV.\n",
    "                row_counter += 1\n",
    "                if response:\n",
    "                    response['queryTime'] = t1 - t0\n",
    "                    writer.writerow(response)\n",
    "                else:\n",
    "                    bad_rows += 1\n",
    "                if row_counter % 1000 == 0:\n",
    "                    row_counter = 0\n",
    "    return output_file, bad_rows\n",
    "\n",
    "def get_otp_response(host_url, input_row) -> tuple:\n",
    "    '''Parse the response from OTP into tuple of values represnting trip attributes'''\n",
    "    response = request_otp(host_url, input_row)\n",
    "    trip = parse_response(response)\n",
    "    if trip:\n",
    "        trip['trip_id'] = input_row['trip_id']\n",
    "    return trip\n",
    "\n",
    "def extract_headers(csv_file):\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        return next(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Time Stamps / Time Interval\n",
    "\n",
    "stratumDict = {\n",
    "    'amPeak':{\n",
    "        'startHour' : 6,\n",
    "        'startMinute' : 30,\n",
    "        'endHour' : 9,\n",
    "        'endMinute' : 00\n",
    "        },\n",
    "    'Saturday':{\n",
    "        'startHour' : 10,\n",
    "        'startMinute' : 00,\n",
    "        'endHour' : 18,\n",
    "        'endMinute' : 00\n",
    "        },\n",
    "    'bh':{\n",
    "        'startHour' : 10,\n",
    "        'startMinute' : 00,\n",
    "        'endHour' : 18,\n",
    "        'endMinute' : 00\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Get day index\n",
    "# Specify the start and end dates\n",
    "start_date = '2024-03-15'\n",
    "end_date = '2024-04-15'\n",
    "\n",
    "# Create a date-time index\n",
    "date_index = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "experiment_dates = pd.DataFrame(index = date_index)\n",
    "\n",
    "experiment_dates['weekday'] = experiment_dates.index.weekday < 5\n",
    "experiment_dates['saturday'] = experiment_dates.index.weekday == 5\n",
    "bank_holidays = ['2024-03-29', '2024-04-01']\n",
    "experiment_dates['bank_holiday'] = experiment_dates.index.isin(pd.to_datetime(bank_holidays))\n",
    "\n",
    "# Get OAs\n",
    "wm_oas = gpd.read_file('data/west_midlands_OAs/west_midlands_OAs.shp')\n",
    "wm_oas = wm_oas[wm_oas['LAD11CD'] == 'E08000026']\n",
    "oa_info = pd.read_csv('data/oa_info.csv')\n",
    "oa_info = oa_info.merge(wm_oas[['OA11CD']], left_on = 'oa_id', right_on = 'OA11CD', how = 'inner')\n",
    "oaLatLon = oa_info[['oa_id','oa_lon','oa_lat']]\n",
    "\n",
    "# Get POIs\n",
    "pois = pd.read_csv('data/POIs/pois_cov.csv', index_col=0)\n",
    "\n",
    "mean = 50\n",
    "std_dev = 20\n",
    "attractivnessDict = {}\n",
    "\n",
    "for pid in list(pois['poi_id']):\n",
    "    random_value = random.normalvariate(mean, std_dev)\n",
    "    attractivnessDict[pid] = max(min(random_value, 100), 0)\n",
    "\n",
    "#Sample 200 random zones\n",
    "oaSample = oa_info.sample(200)[['oa_id','oa_lat','oa_lon']]\n",
    "\n",
    "#POIs\n",
    "POISample = pois.sample(50)\n",
    "POISample['attractiveness'] = POISample['poi_id'].map(attractivnessDict)\n",
    "\n",
    "stratum = 'amPeak'\n",
    "\n",
    "if stratum == 'amPeak':\n",
    "    study_date = experiment_dates[(experiment_dates['weekday']) & (experiment_dates['bank_holiday'] == False)].sample(1).index\n",
    "\n",
    "elif stratum == 'Saturday':\n",
    "    study_date = experiment_dates[(experiment_dates['saturday']) & (experiment_dates['bank_holiday'] == False)].sample(1).index\n",
    "\n",
    "elif stratum == 'bh':\n",
    "    study_date = experiment_dates[experiment_dates['bank_holiday']].sample(1).index\n",
    "\n",
    "# Create Time Domain\n",
    "startHour = stratumDict[stratum]['startHour']\n",
    "startMinute = stratumDict[stratum]['startMinute']\n",
    "endHour = stratumDict[stratum]['endHour']\n",
    "endMinute = stratumDict[stratum]['endMinute']\n",
    "\n",
    "start = datetime(year=2012, month=2, day=25, hour=startHour, minute = startMinute)\n",
    "end = datetime(year=2012, month=2, day=25, hour=endHour, minute = endMinute)\n",
    "diff = end - start\n",
    "minutesInInterval = diff.total_seconds()/60\n",
    "hoursInInterval = minutesInInterval/60\n",
    "\n",
    "timeDomain = []\n",
    "\n",
    "for i in range(50):\n",
    "    randStartTime = start + timedelta(minutes=random.randint(1, int(minutesInInterval)))\n",
    "    if randStartTime not in timeDomain:\n",
    "        timeDomain.append(str(randStartTime.hour).zfill(2)+':'+str(randStartTime.minute).zfill(2))\n",
    "\n",
    "temp_trips_file = 'tempdata/trips_to_route.csv'\n",
    "output_file = open(temp_trips_file, 'w')\n",
    "writer = csv.writer(output_file)\n",
    "writer.writerow(['trip_id','date','time','oa_lat','oa_lon','poi_lat','poi_lon'])\n",
    "\n",
    "#Output trips dataset - output csv with following: \n",
    "trip_id = 0\n",
    "#trip_date = study_date[0].strftime('%m/%d/%Y')\n",
    "trip_date = study_date[0].strftime('%Y-%m-%d')\n",
    "\n",
    "for oind, orow in oaSample.iterrows():\n",
    "    for pind,prow in POISample.iterrows():\n",
    "        for t in timeDomain:            \n",
    "            row = [trip_id,trip_date,t,orow['oa_lat'], orow['oa_lon'],prow['poi_lat'], prow['poi_lon']]\n",
    "            writer.writerow(row)\n",
    "            trip_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'localhost'\n",
    "port = 8080\n",
    "processes = 4\n",
    "otps = 1\n",
    "\n",
    "num_trips = num_rows(temp_trips_file)\n",
    "step_size = get_step_size(num_trips, processes)\n",
    "\n",
    "args = []\n",
    "for i in range(processes):\n",
    "    host_url = f\"http://{host}:{str(port + (i % otps))}\"\n",
    "    offset =i * step_size\n",
    "    arg = (\n",
    "        host_url, \n",
    "        offset, \n",
    "        min(offset+step_size, num_trips), \n",
    "        temp_trips_file, \n",
    "        'tempdata'\n",
    "    )\n",
    "    args.append(arg)\n",
    "\n",
    "rows_complete = multiprocessing.Value('i', 0)\n",
    "\n",
    "t0_routing_cost = time.time()\n",
    "with multiprocessing.Pool(int(processes), initializer=init, initargs=(num_trips, rows_complete)) as pool:\n",
    "    results = pool.starmap(compute_trips, args)\n",
    "t1_routing_cost = time.time()\n",
    "print('Time Taken : {}'.format(t1_routing_cost - t0_routing_cost))\n",
    "\n",
    "files = []\n",
    "bad_rows = 0\n",
    "for f, rows in results:\n",
    "    files.append(f)\n",
    "    bad_rows += rows\n",
    "if bad_rows > 0:\n",
    "    print(f\"{bad_rows} trips were lost during OTP processing ({(bad_rows/num_trips * 100):.2f}%).\")\n",
    "\n",
    "output_file_name = os.path.join('tempdata', 'results_full.csv')\n",
    "files_iterator = iter(files)\n",
    "# Treat the first file differently, so we can extract headers\n",
    "first_file = files[0]\n",
    "headers = extract_headers(first_file)\n",
    "with open(output_file_name, 'w') as output_file:\n",
    "    output_csv = csv.writer(output_file)\n",
    "    output_csv.writerow(headers)\n",
    "    for csv_file in files:\n",
    "        with open(csv_file, newline='') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for line in reader:\n",
    "                output_csv.writerow(line.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10169/4080736814.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['temp_941376.csv','temp_941377.csv','temp_941378.csv','temp_941379.csv','temp_941380.csv','temp_941381.csv','temp_941382.csv','temp_941383.csv','temp_941384.csv','temp_941385.csv','temp_941386.csv','temp_941387.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 0\n",
    "file_counts = []\n",
    "for file in filenames:\n",
    "    temp_data = pd.read_csv('{}/{}'.format('tempdata/',file))\n",
    "    num_records += len(temp_data)\n",
    "    file_counts.append(len(temp_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495729"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'localhost'\n",
    "port = 8080\n",
    "processes = 12\n",
    "otps = 2\n",
    "\n",
    "num_trips = num_rows(temp_trips_file)\n",
    "step_size = get_step_size(num_trips, processes)\n",
    "\n",
    "args = []\n",
    "for i in range(processes):\n",
    "    host_url = f\"http://{host}:{str(port + (i % otps))}\"\n",
    "    offset =i * step_size\n",
    "    arg = (\n",
    "        host_url, \n",
    "        offset, \n",
    "        min(offset+step_size, num_trips), \n",
    "        temp_trips_file, \n",
    "        'tempdata'\n",
    "    )\n",
    "    args.append(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http://localhost:8080',\n",
       "  0,\n",
       "  41659,\n",
       "  'tempdata/trips_to_route_temp.csv',\n",
       "  'tempdata'),\n",
       " ('http://localhost:8081',\n",
       "  41659,\n",
       "  83318,\n",
       "  'tempdata/trips_to_route_temp.csv',\n",
       "  'tempdata'),\n",
       " ('http://localhost:8080',\n",
       "  83318,\n",
       "  124977,\n",
       "  'tempdata/trips_to_route_temp.csv',\n",
       "  'tempdata'),\n",
       " ('http://localhost:8081',\n",
       "  124977,\n",
       "  166636,\n",
       "  'tempdata/trips_to_route_temp.csv',\n",
       "  'tempdata'),\n",
       " ('http://localhost:8080',\n",
       "  166636,\n",
       "  208295,\n",
       "  'tempdata/trips_to_route_temp.csv',\n",
       "  'tempdata'),\n",
       " ('http://localhost:8081',\n",
       "  208295,\n",
       "  249954,\n",
       "  'tempdata/trips_to_route_temp.csv',\n",
       "  'tempdata'),\n",
       " ('http://localhost:8080',\n",
       "  249954,\n",
       "  291613,\n",
       "  'tempdata/trips_to_route_temp.csv',\n",
       "  'tempdata'),\n",
       " ('http://localhost:8081',\n",
       "  291613,\n",
       "  333272,\n",
       "  'tempdata/trips_to_route_temp.csv',\n",
       "  'tempdata'),\n",
       " ('http://localhost:8080',\n",
       "  333272,\n",
       "  374931,\n",
       "  'tempdata/trips_to_route_temp.csv',\n",
       "  'tempdata'),\n",
       " ('http://localhost:8081',\n",
       "  374931,\n",
       "  416590,\n",
       "  'tempdata/trips_to_route_temp.csv',\n",
       "  'tempdata'),\n",
       " ('http://localhost:8080',\n",
       "  416590,\n",
       "  458249,\n",
       "  'tempdata/trips_to_route_temp.csv',\n",
       "  'tempdata'),\n",
       " ('http://localhost:8081',\n",
       "  458249,\n",
       "  499904,\n",
       "  'tempdata/trips_to_route_temp.csv',\n",
       "  'tempdata')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = []\n",
    "for i in args:\n",
    "    num_records.append(i[2] - i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41659,\n",
       " 41659,\n",
       " 41659,\n",
       " 41659,\n",
       " 41659,\n",
       " 41659,\n",
       " 41659,\n",
       " 41659,\n",
       " 41659,\n",
       " 41659,\n",
       " 41659,\n",
       " 41655]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41313,\n",
       " 41311,\n",
       " 41308,\n",
       " 41310,\n",
       " 41313,\n",
       " 41310,\n",
       " 41311,\n",
       " 41313,\n",
       " 41310,\n",
       " 41309,\n",
       " 41310,\n",
       " 41311]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "access",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
