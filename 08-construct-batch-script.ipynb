{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10232/3784863873.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import *\n",
    "from otp_routing_functions import *\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import multiprocessing\n",
    "import statistics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import HDBSCAN\n",
    "import numpy as np\n",
    "\n",
    "def init(trips, complete):\n",
    "    # Make num_trips global in each process.\n",
    "    # This grants read-only access in compute_trips\n",
    "    global num_trips\n",
    "    global rows_complete\n",
    "    num_trips = trips\n",
    "    rows_complete = complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "#Params\n",
    "decay_constant = 1\n",
    "exponent = 0.2\n",
    "#Integer for 1 trip per x minutes. E.g., 1 = a trip every minutes. 5 = 1 trip every 5 minutes\n",
    "# For now go with every minutes but may pull this back\n",
    "# To do - revisit later on\n",
    "temp_resolution = 5\n",
    "num_iterations = 10\n",
    "num_oas = 5\n",
    "num_pois = 50\n",
    "\n",
    "processes = 2\n",
    "otps = 1\n",
    "host = 'localhost'\n",
    "port = 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "\n",
    "# Vectrise distance decay function\n",
    "vfunc = np.vectorize(distance_decay)\n",
    "\n",
    "stratumDict = {\n",
    "    'wdam':{\n",
    "        'startHour' : 6,\n",
    "        'startMinute' : 30,\n",
    "        'endHour' : 8,\n",
    "        'endMinute' : 30\n",
    "        },\n",
    "    'wdpm':{\n",
    "        'startHour' : 16,\n",
    "        'startMinute' : 00,\n",
    "        'endHour' : 18,\n",
    "        'endMinute' : 30\n",
    "        },\n",
    "    'sat':{\n",
    "        'startHour' : 10,\n",
    "        'startMinute' : 00,\n",
    "        'endHour' : 18,\n",
    "        'endMinute' : 00\n",
    "        },\n",
    "    'bh':{\n",
    "        'startHour' : 10,\n",
    "        'startMinute' : 00,\n",
    "        'endHour' : 18,\n",
    "        'endMinute' : 00\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Get day index\n",
    "# Specify the start and end dates\n",
    "start_date = '2024-03-15'\n",
    "end_date = '2024-04-15'\n",
    "\n",
    "# Create a date-time index\n",
    "date_index = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "experiment_dates = pd.DataFrame(index = date_index)\n",
    "\n",
    "experiment_dates['weekday'] = experiment_dates.index.weekday < 5\n",
    "experiment_dates['saturday'] = experiment_dates.index.weekday == 5\n",
    "bank_holidays = ['2024-03-29', '2024-04-01']\n",
    "experiment_dates['bank_holiday'] = experiment_dates.index.isin(pd.to_datetime(bank_holidays))\n",
    "\n",
    "# Get OAs\n",
    "wm_oas = gpd.read_file('data/west_midlands_OAs/west_midlands_OAs.shp')\n",
    "wm_oas = wm_oas[wm_oas['LAD11CD'] == 'E08000026']\n",
    "oa_info = pd.read_csv('data/oa_info.csv')\n",
    "oa_info = oa_info.merge(wm_oas[['OA11CD']], left_on = 'oa_id', right_on = 'OA11CD', how = 'inner')\n",
    "oaLatLon = oa_info[['oa_id','oa_lon','oa_lat']]\n",
    "\n",
    "# Get POIs\n",
    "pois = pd.read_csv('data/POIs/pois_cov.csv', index_col=0)\n",
    "\n",
    "# Generate POI attractiveness using gaussian randomness\n",
    "\n",
    "mean = 50\n",
    "std_dev = 20\n",
    "attractivnessDict = {}\n",
    "\n",
    "for pid in list(pois['poi_id']):\n",
    "    random_value = random.normalvariate(mean, std_dev)\n",
    "    attractivnessDict[pid] = max(min(random_value, 100), 0)\n",
    "\n",
    "\n",
    "# Create tracking matrices\n",
    "\n",
    "performance = {}\n",
    "processing_times = {}\n",
    "for i in range(num_iterations):\n",
    "    performance[i] = {}\n",
    "    processing_times[i] = {}\n",
    "    for k in list(stratumDict.keys()):\n",
    "        performance[i][k] = {}\n",
    "        processing_times[i][k] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample 200 random zones\n",
    "oaSample = oa_info.sample(num_oas)[['oa_id','oa_lat','oa_lon']]\n",
    "\n",
    "#POIs\n",
    "POISample = pois.sample(num_pois)\n",
    "POISample['attractiveness'] = POISample['poi_id'].map(attractivnessDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken : 176.20800876617432\n",
      "Running Trip Gen Gravity\n",
      "Time Taken : 271.87348103523254\n",
      "Running Trip Gen Gravity\n"
     ]
    }
   ],
   "source": [
    "for stratum in stratumDict.keys():\n",
    "    if stratum == 'wdam' or stratum == 'wdpm' :\n",
    "        study_date = experiment_dates[(experiment_dates['weekday']) & (experiment_dates['bank_holiday'] == False)].sample(1).index\n",
    "\n",
    "    elif stratum == 'sat':\n",
    "        study_date = experiment_dates[(experiment_dates['saturday']) & (experiment_dates['bank_holiday'] == False)].sample(1).index\n",
    "\n",
    "    elif stratum == 'bh':\n",
    "        study_date = experiment_dates[experiment_dates['bank_holiday']].sample(1).index\n",
    "\n",
    "    # Create Time Domain\n",
    "    startHour = stratumDict[stratum]['startHour']\n",
    "    startMinute = stratumDict[stratum]['startMinute']\n",
    "    endHour = stratumDict[stratum]['endHour']\n",
    "    endMinute = stratumDict[stratum]['endMinute']\n",
    "\n",
    "    start = datetime(year=2012, month=2, day=25, hour=startHour, minute = startMinute)\n",
    "    end = datetime(year=2012, month=2, day=25, hour=endHour, minute = endMinute)\n",
    "    diff = end - start\n",
    "    minutesInInterval = diff.total_seconds()/60\n",
    "    hoursInInterval = minutesInInterval/60\n",
    "\n",
    "    num_trips = int((60 / temp_resolution) * hoursInInterval)\n",
    "\n",
    "    timeDomain = []\n",
    "\n",
    "    for i in range(num_trips):\n",
    "        randStartTime = start + timedelta(minutes=random.randint(1, int(minutesInInterval)))\n",
    "        if randStartTime not in timeDomain:\n",
    "            timeDomain.append(str(randStartTime.hour).zfill(2)+':'+str(randStartTime.minute).zfill(2))\n",
    "\n",
    "    # Output trips to CSV\n",
    "\n",
    "    temp_trips_file = 'tempdata/trips_to_route.csv'\n",
    "    output_file = open(temp_trips_file, 'w')\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(['oa_id','poi_id','trip_id','date','time','oa_lat','oa_lon','poi_lat','poi_lon'])\n",
    "\n",
    "    #Output trips dataset - output csv with following: \n",
    "    trip_id = 0\n",
    "    #trip_date = study_date[0].strftime('%m/%d/%Y')\n",
    "    trip_date = study_date[0].strftime('%Y-%m-%d')\n",
    "\n",
    "    for oind, orow in oaSample.iterrows():\n",
    "        for pind,prow in POISample.iterrows():\n",
    "            for t in timeDomain:            \n",
    "                row = [orow['oa_id'],prow['poi_id'],trip_id,trip_date,t,orow['oa_lat'], orow['oa_lon'],prow['poi_lat'], prow['poi_lon']]\n",
    "                writer.writerow(row)\n",
    "                trip_id += 1\n",
    "\n",
    "    # Cost trips on OTP using parallelisation\n",
    "\n",
    "    num_trips = num_rows(temp_trips_file)\n",
    "    step_size = get_step_size(num_trips, processes)\n",
    "\n",
    "    args = []\n",
    "    for i in range(processes):\n",
    "        host_url = f\"http://{host}:{str(port + (i % otps))}\"\n",
    "        offset =i * step_size\n",
    "        arg = (\n",
    "            host_url, \n",
    "            offset, \n",
    "            min(offset+step_size, num_trips), \n",
    "            temp_trips_file, \n",
    "            'tempdata'\n",
    "        )\n",
    "        args.append(arg)\n",
    "\n",
    "    rows_complete = multiprocessing.Value('i', 0)\n",
    "\n",
    "    t0_routing_cost = time.time()\n",
    "    with multiprocessing.Pool(int(processes), initializer=init, initargs=(num_trips, rows_complete)) as pool:\n",
    "        results = pool.starmap(compute_trips, args)\n",
    "    t1_routing_cost = time.time()\n",
    "    print('Time Taken : {}'.format(t1_routing_cost - t0_routing_cost))\n",
    "\n",
    "    files = []\n",
    "    bad_rows = 0\n",
    "    for f, rows in results:\n",
    "        files.append(f)\n",
    "        bad_rows += rows\n",
    "    if bad_rows > 0:\n",
    "        print(f\"{bad_rows} trips were lost during OTP processing ({(bad_rows/num_trips * 100):.2f}%).\")\n",
    "\n",
    "    output_file_name = os.path.join('tempdata', 'results_full.csv')\n",
    "    files_iterator = iter(files)\n",
    "    # Treat the first file differently, so we can extract headers\n",
    "    first_file = files[0]\n",
    "    headers = extract_headers(first_file)\n",
    "    with open(output_file_name, 'w') as output_file:\n",
    "        output_csv = csv.writer(output_file)\n",
    "        output_csv.writerow(headers)\n",
    "        for csv_file in files:\n",
    "            with open(csv_file, newline='') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                for line in reader:\n",
    "                    output_csv.writerow(line.values())\n",
    "\n",
    "\n",
    "    initial_trips = pd.read_csv('tempdata/trips_to_route.csv').set_index('trip_id')\n",
    "    trips = pd.read_csv('tempdata/results_full.csv').set_index('trip_id')\n",
    "    trips = trips.merge(initial_trips[['poi_id','oa_id','time']],right_index=True, left_index=True)\n",
    "\n",
    "    #OA-POI Index Dict\n",
    "    oa_poi_id_dict = {}\n",
    "\n",
    "    for oa_id in list(oaSample['oa_id']):\n",
    "        oa_poi_id_dict[oa_id] = {}\n",
    "        for poi_id in list(POISample['poi_id']):\n",
    "            oa_poi_id_dict[oa_id][poi_id] = list(trips[(trips['oa_id'] == oa_id) & (trips['poi_id'] == poi_id)].index)\n",
    "\n",
    "    peformance_mx = pd.DataFrame(index = list(oaSample['oa_id']))\n",
    "    costing_mx = pd.DataFrame(index = list(oaSample['oa_id']))\n",
    "    computing_times = {}\n",
    "\n",
    "    #Compute generalised access cost\n",
    "    trips['gac'] = (( 1.5 * (trips['total_time'])) - (0.5 * trips['transit_time']) + ((trips['fare'] * 3600) / 6.7) + (10 * trips['num_transfers'])) / 60\n",
    "    trips['att'] = trips['poi_id'].map(attractivnessDict)\n",
    "\n",
    "    #Gravity Model Ground Truth\n",
    "    trips['dist decay'] = vfunc(np.array(trips['gac']),decay_constant, exponent)\n",
    "    trips['grav'] = trips['dist decay'] * trips['att']\n",
    "    gravity = trips.groupby('oa_id').sum()['grav']\n",
    "\n",
    "    peformance_mx['GM'] = gravity\n",
    "    costing_mx['GM'] = trips.groupby('oa_id').sum()['queryTime']\n",
    "\n",
    "    # Run KNN\n",
    "\n",
    "    for k in [5,15,25]:\n",
    "\n",
    "        processing_time = 0\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        knn = NearestNeighbors(n_neighbors=k)\n",
    "        knn.fit(POISample[['poi_lon','poi_lat']].values)\n",
    "        kResList = []\n",
    "        countOrigins = 0\n",
    "\n",
    "        t1 = time.time()\n",
    "        processing_time += (t1 - t0)\n",
    "\n",
    "        for oind, orow in oaSample.iterrows():\n",
    "            t0 = time.time()\n",
    "            distances, indices = knn.kneighbors(orow[['oa_lon','oa_lat']].values.reshape(1, -1))\n",
    "            t1 = time.time()\n",
    "            processing_time += (t1 - t0)\n",
    "            \n",
    "            oCosts = []\n",
    "            oTrips = 0\n",
    "            oTimes = 0\n",
    "\n",
    "            for i in indices[0]:\n",
    "                pid = POISample.iloc[i]['poi_id']\n",
    "                #trips_sample = trips[(trips['oa_id'] == orow['oa_id']) & (trips['poi_id'] == pid)]\n",
    "                trips_sample = trips.loc[oa_poi_id_dict[orow['oa_id']][pid]]\n",
    "                oCosts = oCosts + list(trips_sample['gac'])\n",
    "                oTrips += len(trips_sample)\n",
    "                oTimes += trips_sample['queryTime'].sum()\n",
    "\n",
    "            kResAppend = {}\n",
    "            kResAppend['oa_id'] = orow['oa_id']\n",
    "            kResAppend['score'] = statistics.mean(oCosts)\n",
    "            kResAppend['times'] = oTimes\n",
    "            kResList.append(kResAppend)\n",
    "\n",
    "        knnres = pd.DataFrame(kResList).set_index('oa_id')\n",
    "\n",
    "        peformance_mx['knn_{}'.format(k)] = knnres['score']\n",
    "        costing_mx['KNN_{}'.format(k)] = knnres['times']\n",
    "        computing_times['KNN_{}'.format(k)] = processing_time\n",
    "\n",
    "    #Run k-means\n",
    "\n",
    "    for num_clusters in [3,5,7,9]:\n",
    "\n",
    "        processing_time = 0\n",
    "\n",
    "        t0 = time.time()\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=\"auto\").fit(POISample[['poi_lon','poi_lat']].values)\n",
    "        # Get cluster labels and centroids\n",
    "        cluster_labels = kmeans.labels_\n",
    "        t1 = time.time()\n",
    "        processing_time += (t1 - t0)\n",
    "\n",
    "        POISample['cluster'] = cluster_labels\n",
    "\n",
    "        flows_df_list = []\n",
    "\n",
    "        for cluster in set(cluster_labels):\n",
    "            t0 = time.time()\n",
    "            points_in_cluster = np.where(cluster_labels == cluster)[0]\n",
    "            centroid = [POISample[POISample['cluster'] == cluster]['poi_lon'].mean(),POISample[POISample['cluster'] == cluster]['poi_lat'].mean()]\n",
    "            distances_to_centroid = np.linalg.norm(POISample[['poi_lon','poi_lat']].values[points_in_cluster] - POISample[['poi_lon','poi_lat']].values[points_in_cluster].mean(axis = 0), axis=1)\n",
    "            # Find the index of the point closest to the centroid\n",
    "            closest_point_index = points_in_cluster[np.argmin(distances_to_centroid)]\n",
    "            flow_id = POISample.iloc[closest_point_index]['poi_id']\n",
    "            t1 = time.time()\n",
    "            processing_time += (t1 - t0)\n",
    "            oa_count = 0\n",
    "            for oa in list(oaSample['oa_id']):\n",
    "                oa_count += 1\n",
    "                flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                query_times = list(flow_gac['queryTime'])\n",
    "\n",
    "                for poi in points_in_cluster:\n",
    "                    flow_append = flow_gac.copy()\n",
    "                    if poi != closest_point_index:\n",
    "                        flow_append['queryTime'] = 0\n",
    "                    else:\n",
    "                        flow_append['queryTime'] = query_times\n",
    "                    flow_append['poi_id'] = POISample.iloc[poi]['poi_id']\n",
    "                    flows_df_list.append(flow_append)\n",
    "\n",
    "        flows_df = pd.concat(flows_df_list, ignore_index=True)\n",
    "        flows_df['att'] = flows_df['poi_id'].map(attractivnessDict)\n",
    "        flows_df['dist decay'] = vfunc(np.array(flows_df['gac']),decay_constant, exponent)\n",
    "        flows_df['grav'] = flows_df['dist decay'] * flows_df['att']\n",
    "        peformance_mx['kmean_{}'.format(num_clusters)] = flows_df.groupby('oa_id').sum()['grav']\n",
    "        costing_mx['kmean_{}'.format(num_clusters)] = flows_df.groupby('oa_id').sum()['queryTime']\n",
    "        computing_times['kmean_{}'.format(num_clusters)] = processing_time\n",
    "\n",
    "    #DBSCAN\n",
    "\n",
    "    eps_test = [0.01,0.02,0.03]\n",
    "    min_samples = [1,3,5]\n",
    "\n",
    "    for ep in eps_test:\n",
    "        for ms in min_samples:\n",
    "            processing_time = 0\n",
    "\n",
    "            t0 = time.time()\n",
    "            dbscan = DBSCAN(eps=ep, min_samples=ms).fit(POISample[['poi_lon','poi_lat']].values)\n",
    "\n",
    "            # Get cluster labels and centroids\n",
    "            cluster_labels = dbscan.labels_\n",
    "            POISample['cluster'] = cluster_labels\n",
    "            t1 = time.time()\n",
    "            processing_time += (t1 - t0)\n",
    "            flows_df_list = []\n",
    "\n",
    "            for cluster in set(cluster_labels):\n",
    "                if cluster == -1:\n",
    "                    for c_ind in np.where(cluster_labels == cluster)[0]:\n",
    "                        flow_id = POISample.iloc[c_ind]['poi_id']\n",
    "\n",
    "                        for oa in list(oaSample['oa_id']):\n",
    "                            flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                            flows_df_list.append(flow_gac)\n",
    "                else:\n",
    "                    t0 = time.time()\n",
    "                    points_in_cluster = np.where(cluster_labels == cluster)[0]\n",
    "                    centroid = [POISample[POISample['cluster'] == cluster]['poi_lon'].mean(),POISample[POISample['cluster'] == cluster]['poi_lat'].mean()]\n",
    "                    distances_to_centroid = np.linalg.norm(POISample[['poi_lon','poi_lat']].values[points_in_cluster] - POISample[['poi_lon','poi_lat']].values[points_in_cluster].mean(axis = 0), axis=1)\n",
    "                    # Find the index of the point closest to the centroid\n",
    "                    closest_point_index = points_in_cluster[np.argmin(distances_to_centroid)]\n",
    "                    flow_id = POISample.iloc[closest_point_index]['poi_id']\n",
    "                    t1 = time.time()\n",
    "                    processing_time += (t1 - t0)\n",
    "                    for oa in list(oaSample['oa_id']):\n",
    "                        flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                        query_times = list(flow_gac['queryTime'])\n",
    "                        for poi in points_in_cluster:\n",
    "                            flow_append = flow_gac.copy()\n",
    "                            if poi != closest_point_index:\n",
    "                                flow_append['queryTime'] = 0\n",
    "                            else:\n",
    "                                flow_append['queryTime'] = query_times\n",
    "                            flow_append['poi_id'] = POISample.iloc[poi]['poi_id']\n",
    "                            flows_df_list.append(flow_append)\n",
    "\n",
    "            flows_df = pd.concat(flows_df_list, ignore_index=True)\n",
    "            flows_df['att'] = flows_df['poi_id'].map(attractivnessDict)\n",
    "            flows_df['dist decay'] = vfunc(np.array(flows_df['gac']),decay_constant, exponent)\n",
    "            flows_df['grav'] = flows_df['dist decay'] * flows_df['att']\n",
    "            peformance_mx['dbscan_{}_{}'.format(ep,ms)] = flows_df.groupby('oa_id').sum()['grav']\n",
    "            costing_mx['dbscan_{}_{}'.format(ep,ms)] = flows_df.groupby('oa_id').sum()['queryTime']\n",
    "            computing_times['dbscan_{}_{}'.format(ep,ms)] = processing_time\n",
    "\n",
    "    #HDBSCAN\n",
    "\n",
    "    min_clusters = [3,5,7]\n",
    "    min_sample_tests = [1,3,5,7]\n",
    "\n",
    "    for mc in min_clusters:\n",
    "        for ms in min_sample_tests:\n",
    "\n",
    "            processing_time = 0\n",
    "\n",
    "            t0 = time.time()\n",
    "            hdbscan = HDBSCAN(min_cluster_size = mc, min_samples=ms).fit(POISample[['poi_lon','poi_lat']].values)\n",
    "\n",
    "            # Get cluster labels and centroids\n",
    "            cluster_labels = hdbscan.labels_\n",
    "            POISample['cluster'] = cluster_labels\n",
    "            t1 = time.time()\n",
    "            processing_time += (t1 - t0)\n",
    "\n",
    "            flows_df_list = []\n",
    "\n",
    "            for cluster in set(cluster_labels):\n",
    "                if cluster == -1:\n",
    "                    for c_ind in np.where(cluster_labels == cluster)[0]:\n",
    "                        flow_id = POISample.iloc[c_ind]['poi_id']\n",
    "\n",
    "                        for oa in list(oaSample['oa_id']):\n",
    "                            flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                            flows_df_list.append(flow_gac)\n",
    "                else:\n",
    "                    t0 = time.time()\n",
    "                    points_in_cluster = np.where(cluster_labels == cluster)[0]\n",
    "                    centroid = [POISample[POISample['cluster'] == cluster]['poi_lon'].mean(),POISample[POISample['cluster'] == cluster]['poi_lat'].mean()]\n",
    "                    distances_to_centroid = np.linalg.norm(POISample[['poi_lon','poi_lat']].values[points_in_cluster] - POISample[['poi_lon','poi_lat']].values[points_in_cluster].mean(axis = 0), axis=1)\n",
    "                    # Find the index of the point closest to the centroid\n",
    "                    closest_point_index = points_in_cluster[np.argmin(distances_to_centroid)]\n",
    "                    flow_id = POISample.iloc[closest_point_index]['poi_id']\n",
    "                    t1 = time.time()\n",
    "                    processing_time += (t1 - t0)\n",
    "                    for oa in list(oaSample['oa_id']):\n",
    "                        flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                        query_times = list(flow_gac['queryTime'])\n",
    "                        for poi in points_in_cluster:\n",
    "                            flow_append = flow_gac.copy()\n",
    "                            if poi != closest_point_index:\n",
    "                                flow_append['queryTime'] = 0\n",
    "                            else:\n",
    "                                flow_append['queryTime'] = query_times\n",
    "                            flow_append['poi_id'] = POISample.iloc[poi]['poi_id']\n",
    "                            flows_df_list.append(flow_append)\n",
    "\n",
    "            flows_df = pd.concat(flows_df_list, ignore_index=True)\n",
    "            flows_df['att'] = flows_df['poi_id'].map(attractivnessDict)\n",
    "            flows_df['dist decay'] = vfunc(np.array(flows_df['gac']),decay_constant, exponent)\n",
    "            flows_df['grav'] = flows_df['dist decay'] * flows_df['att']\n",
    "            peformance_mx['hdbscan_{}_{}'.format(mc,ms)] = flows_df.groupby('oa_id').sum()['grav']\n",
    "            costing_mx['hdbscan_{}_{}'.format(mc,ms)] = flows_df.groupby('oa_id').sum()['queryTime']\n",
    "            computing_times['hdbscan_{}_{}'.format(mc,ms)] = processing_time\n",
    "\n",
    "\n",
    "    # Get flow\n",
    "    # Cluster flows\n",
    "\n",
    "    minflows = [3,5,7]\n",
    "    minclusters = [5,10,15]\n",
    "    minsamples = [5,10,15]\n",
    "\n",
    "    for mf in minflows:\n",
    "        X, o_index, d_index, flows, flows_index = get_flow_dist_mx(oaSample,POISample,mf,flowProx)\n",
    "        for mc in minclusters:\n",
    "            for ms in minsamples:\n",
    "                \n",
    "                processing_time = 0\n",
    "\n",
    "                t0 = time.time()\n",
    "                hdb = HDBSCAN(min_cluster_size=mc, min_samples=ms, metric=getminreach).fit(X)\n",
    "                cluster_labels = hdb.labels_\n",
    "                t1 = time.time()\n",
    "                processing_time += (t1 - t0)\n",
    "                flows_df_list = []\n",
    "                for cluster in set(cluster_labels):\n",
    "                    if cluster == -1:\n",
    "                        cluster_indeces = np.where(cluster_labels==cluster)[0]\n",
    "                        flow_inds = list(set([flows_index[i] for i in cluster_indeces]))\n",
    "                        for f in flow_inds:\n",
    "                            flow_gac = trips.loc[oa_poi_id_dict[f[0]][f[1]]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                            flows_df_list.append(flow_gac)\n",
    "                    else:\n",
    "                        t0 = time.time()\n",
    "                        cluster_indeces = np.where(cluster_labels==cluster)[0]\n",
    "                        #Select origin of best flow\n",
    "                        oas_in_flow = list(set([o_index[i] for i in cluster_indeces]))\n",
    "                        distances_to_centroid = np.linalg.norm(oaSample.set_index('oa_id').loc[oas_in_flow].values - oaSample.set_index('oa_id').loc[oas_in_flow].values.mean(axis = 0),axis=1)\n",
    "                        flow_oa = oas_in_flow[np.argmin(distances_to_centroid)]\n",
    "                        #Select destination of best flow\n",
    "                        poi_ids = list(set([d_index[i] for i in cluster_indeces]))\n",
    "                        distances_to_centroid = np.linalg.norm(POISample.set_index('poi_id').loc[poi_ids][['poi_lon','poi_lat']].values - POISample.set_index('poi_id').loc[poi_ids][['poi_lon','poi_lat']].values.mean(axis = 0),axis = 1)\n",
    "                        flow_poi = poi_ids[np.argmin(distances_to_centroid)]\n",
    "                        t1 = time.time()\n",
    "                        processing_time += (t1 - t0)\n",
    "                        #measure GAC for all time steps\n",
    "                        flow_gac = trips.loc[oa_poi_id_dict[flow_oa][flow_poi]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                        query_times = list(flow_gac['queryTime'])\n",
    "                        flow_inds = list(set([flows_index[i] for i in cluster_indeces]))\n",
    "                        for f in flow_inds:\n",
    "                            flow_append = flow_gac.copy()\n",
    "                            if f[1] != flow_poi:\n",
    "                                flow_append['queryTime'] = 0\n",
    "                            else:\n",
    "                                flow_append['queryTime'] = query_times\n",
    "                            flow_append['oa_id'] = f[0]\n",
    "                            flow_append['poi_id'] = f[1]\n",
    "                            flows_df_list.append(flow_append)\n",
    "                flows_df = pd.concat(flows_df_list, ignore_index=True)\n",
    "                flows_df['att'] = flows_df['poi_id'].map(attractivnessDict)\n",
    "                flows_df['dist decay'] = vfunc(np.array(flows_df['gac']),decay_constant, exponent)\n",
    "                flows_df['grav'] = flows_df['dist decay'] * flows_df['att']\n",
    "                peformance_mx['flowhdbscan_{}_{}_{}'.format(mf,mc,ms)] = flows_df.groupby('oa_id').sum()['grav']\n",
    "                costing_mx['flowhdbscan_{}_{}_{}'.format(mf,mc,ms)] = flows_df.groupby('oa_id').sum()['queryTime']\n",
    "                computing_times['flowhdbscan_{}_{}_{}'.format(mf,mc,ms)] = processing_time\n",
    "\n",
    "\n",
    "    # Gravit Trip Generator\n",
    "\n",
    "    distMxList = []\n",
    "\n",
    "    for i,r in oaSample.iterrows():\n",
    "        for i_, r_ in POISample.iterrows():\n",
    "            rowAppend = {}\n",
    "            rowAppend['oa'] = r['oa_id']\n",
    "            rowAppend['poi'] = r_['poi_id']\n",
    "            rowAppend['dist'] = haversine_distance(r['oa_lon'], r['oa_lat'], r_['poi_lon'], r_['poi_lat'])\n",
    "            distMxList.append(rowAppend)\n",
    "\n",
    "    distMx = pd.DataFrame(distMxList)\n",
    "    distMx['att'] = distMx['poi'].map(attractivnessDict)\n",
    "\n",
    "    distsDecay = []\n",
    "    for i in np.array(distMx['dist']):\n",
    "        distsDecay.append(distance_decay(i, decay_constant, exponent))\n",
    "\n",
    "    distMx['decay'] = distsDecay\n",
    "    distMx['grav'] = distMx['decay'] * distMx['att']\n",
    "    distMx['gravN'] = (distMx['grav'] - distMx['grav'].min()) / (distMx['grav'].max() - distMx['grav'].min())\n",
    "\n",
    "    distMx = distMx.merge(trips.groupby(['oa_id','poi_id']).count()['departure_time'].rename('tripCount'), left_on = ['oa','poi'],right_index = True)\n",
    "    distMx['tripsSample'] = distMx['tripCount'] * distMx['gravN']\n",
    "\n",
    "    resultsList = []\n",
    "    countOrigins = 0\n",
    "\n",
    "    print('Running Trip Gen Gravity')\n",
    "\n",
    "    for o in list(oaSample['oa_id']):\n",
    "        countOrigins += 1\n",
    "        if countOrigins % 20 == 0:\n",
    "            print(countOrigins)\n",
    "        oCosts = []\n",
    "        oTrips = 0\n",
    "        oTimes = 0\n",
    "        for p in list(POISample['poi_id']):\n",
    "            if len(oa_poi_id_dict[o][p]) > 0:\n",
    "                numSample = int(distMx[(distMx['oa'] == o) & (distMx['poi'] == p)]['tripsSample'].values[0])\n",
    "                tripSample = trips.loc[oa_poi_id_dict[o][p]].sample(numSample)\n",
    "                oCosts = oCosts + list(tripSample['gac'])\n",
    "                oTrips += numSample\n",
    "                oTimes += tripSample['queryTime'].sum()\n",
    "        rowAppend = {}\n",
    "        rowAppend['OA'] = o\n",
    "        rowAppend['GTG'] = statistics.mean(oCosts)\n",
    "        rowAppend['GTG_Trips'] = oTrips\n",
    "        rowAppend['GTG_Times'] = oTimes\n",
    "        resultsList.append(rowAppend)\n",
    "\n",
    "    tripGenGrav = pd.DataFrame(resultsList).set_index('OA')\n",
    "\n",
    "    peformance_mx['g-tgm'] = tripGenGrav['GTG']\n",
    "    costing_mx['g-tgm'] = tripGenGrav['GTG_Times']\n",
    "\n",
    "    performance[it][stratum] = peformance_mx\n",
    "    processing_times[it][stratum] = costing_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Left to do:\n",
    "\n",
    "- Output dictionary objects\n",
    "- Finalise collection of processing times including flow matrix\n",
    "- Include delete all file statement\n",
    "- Put into batch script and paramaterise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "access",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
