{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8264/2849659053.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import *\n",
    "from otp_routing_functions import *\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import multiprocessing\n",
    "import statistics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import HDBSCAN\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Parameters\n",
    "decay_constant = config['decay_constant']\n",
    "exponent = config['exponent']\n",
    "#Integer for 1 trip per x minutes. E.g., 1 = a trip every minutes. 5 = 1 trip every 5 minutes\n",
    "# For now go with every minutes but may pull this back\n",
    "# To do - revisit later on\n",
    "temp_resolution = config['temp_resolution']\n",
    "num_iterations = config['num_iterations']\n",
    "num_oas = config['num_oas']\n",
    "num_pois = config['num_pois']\n",
    "processes = config['processes']\n",
    "otps = config['otps']\n",
    "host = config['host']\n",
    "port = config['port']\n",
    "\n",
    "# Vectrise distance decay function\n",
    "vfunc = np.vectorize(distance_decay)\n",
    "\n",
    "stratumDict = {\n",
    "    'wdam':{\n",
    "        'startHour' : 6,\n",
    "        'startMinute' : 30,\n",
    "        'endHour' : 8,\n",
    "        'endMinute' : 30\n",
    "        },\n",
    "    'wdpm':{\n",
    "        'startHour' : 16,\n",
    "        'startMinute' : 00,\n",
    "        'endHour' : 18,\n",
    "        'endMinute' : 30\n",
    "        },\n",
    "    'sat':{\n",
    "        'startHour' : 10,\n",
    "        'startMinute' : 00,\n",
    "        'endHour' : 18,\n",
    "        'endMinute' : 00\n",
    "        },\n",
    "    'bh':{\n",
    "        'startHour' : 10,\n",
    "        'startMinute' : 00,\n",
    "        'endHour' : 18,\n",
    "        'endMinute' : 00\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Get day index\n",
    "# Specify the start and end dates\n",
    "start_date = '2024-03-15'\n",
    "end_date = '2024-04-15'\n",
    "\n",
    "# Create a date-time index\n",
    "date_index = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "experiment_dates = pd.DataFrame(index = date_index)\n",
    "experiment_dates['weekday'] = experiment_dates.index.weekday < 5\n",
    "experiment_dates['saturday'] = experiment_dates.index.weekday == 5\n",
    "bank_holidays = ['2024-03-29', '2024-04-01']\n",
    "experiment_dates['bank_holiday'] = experiment_dates.index.isin(pd.to_datetime(bank_holidays))\n",
    "\n",
    "# Get OAs\n",
    "wm_oas = gpd.read_file('data/west_midlands_OAs/west_midlands_OAs.shp')\n",
    "wm_oas = wm_oas[wm_oas['LAD11CD'] == 'E08000026']\n",
    "oa_info = pd.read_csv('data/oa_info.csv')\n",
    "oa_info = oa_info.merge(wm_oas[['OA11CD']], left_on = 'oa_id', right_on = 'OA11CD', how = 'inner')\n",
    "oaLatLon = oa_info[['oa_id','oa_lon','oa_lat']]\n",
    "\n",
    "# Get POIs\n",
    "pois = pd.read_csv('data/POIs/pois_cov.csv', index_col=0)\n",
    "\n",
    "# Generate POI attractiveness using gaussian randomness\n",
    "\n",
    "mean = 50\n",
    "std_dev = 20\n",
    "attractivnessDict = {}\n",
    "\n",
    "for pid in list(pois['poi_id']):\n",
    "    random_value = random.normalvariate(mean, std_dev)\n",
    "    attractivnessDict[pid] = max(min(random_value, 100), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tracking matrices\n",
    "performance = {}\n",
    "processing_times = {}\n",
    "exp_meta_data = {}\n",
    "for i in range(num_iterations):\n",
    "    performance[i] = {}\n",
    "    processing_times[i] = {}\n",
    "    exp_meta_data[i] = {}\n",
    "    for k in list(stratumDict.keys()):\n",
    "        performance[i][k] = {}\n",
    "        processing_times[i][k] = {}\n",
    "        exp_meta_data[i][k] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for it in range(num_iterations):\n",
    "for it in range(num_iterations):\n",
    "    for stratum in stratumDict.keys():\n",
    "        initial_trips = pd.read_csv('results/trips_samples/trips_to_route_{}_{}.csv'.format(it,stratum)).set_index('trip_id')\n",
    "        trips = pd.read_csv('results/trips_samples/results_full_{}_{}.csv'.format(it,stratum)).set_index('trip_id')\n",
    "        trips = trips.merge(initial_trips[['poi_id','oa_id','time']],right_index=True, left_index=True)\n",
    "\n",
    "        #Sample 200 random zones\n",
    "        oaSample = oa_info[oa_info['oa_id'].isin(list(set(trips['oa_id'])))][['oa_id','oa_lat','oa_lon']]\n",
    "\n",
    "        #POIs\n",
    "        POISample = pois[pois['poi_id'].isin(list(set(trips['poi_id'])))]\n",
    "        POISample['attractiveness'] = POISample['poi_id'].map(attractivnessDict)\n",
    "\n",
    "        exp_meta_data[it][stratum]['num_trips_initial'] = len(initial_trips)\n",
    "        exp_meta_data[it][stratum]['num_trips_costed'] = len(trips)\n",
    "\n",
    "        #OA-POI Index Dict\n",
    "        oa_poi_id_dict = {}\n",
    "\n",
    "        for oa_id in list(oaSample['oa_id']):\n",
    "            oa_poi_id_dict[oa_id] = {}\n",
    "            for poi_id in list(POISample['poi_id']):\n",
    "                oa_poi_id_dict[oa_id][poi_id] = list(trips[(trips['oa_id'] == oa_id) & (trips['poi_id'] == poi_id)].index)\n",
    "\n",
    "        peformance_mx = pd.DataFrame(index = list(oaSample['oa_id']))\n",
    "        costing_mx = pd.DataFrame(index = list(oaSample['oa_id']))\n",
    "        computing_times = {}\n",
    "\n",
    "        #Compute generalised access cost\n",
    "        trips['gac'] = (( 1.5 * (trips['total_time'])) - (0.5 * trips['transit_time']) + ((trips['fare'] * 3600) / 6.7) + (10 * trips['num_transfers'])) / 60\n",
    "        trips['att'] = trips['poi_id'].map(attractivnessDict)\n",
    "\n",
    "        #Gravity Model Ground Truth\n",
    "        trips['dist decay'] = vfunc(np.array(trips['gac']),decay_constant, exponent)\n",
    "        trips['grav'] = trips['dist decay'] * trips['att']\n",
    "        gravity = trips.groupby('oa_id').sum()['grav']\n",
    "\n",
    "        peformance_mx['GM'] = gravity\n",
    "        costing_mx['GM'] = trips.groupby('oa_id').sum()['queryTime']\n",
    "\n",
    "        # Run KNN\n",
    "\n",
    "        print('process knn results')\n",
    "\n",
    "        for k in [5,15,25]:\n",
    "\n",
    "            processing_time = 0\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            knn = NearestNeighbors(n_neighbors=k)\n",
    "            knn.fit(POISample[['poi_lon','poi_lat']].values)\n",
    "            kResList = []\n",
    "            countOrigins = 0\n",
    "\n",
    "            t1 = time.time()\n",
    "            processing_time += (t1 - t0)\n",
    "\n",
    "            for oind, orow in oaSample.iterrows():\n",
    "                t0 = time.time()\n",
    "                distances, indices = knn.kneighbors(orow[['oa_lon','oa_lat']].values.reshape(1, -1))\n",
    "                t1 = time.time()\n",
    "                processing_time += (t1 - t0)\n",
    "                \n",
    "                oCosts = []\n",
    "                oTrips = 0\n",
    "                oTimes = 0\n",
    "\n",
    "                for i in indices[0]:\n",
    "                    pid = POISample.iloc[i]['poi_id']\n",
    "                    #trips_sample = trips[(trips['oa_id'] == orow['oa_id']) & (trips['poi_id'] == pid)]\n",
    "                    trips_sample = trips.loc[oa_poi_id_dict[orow['oa_id']][pid]]\n",
    "                    oCosts = oCosts + list(trips_sample['gac'])\n",
    "                    oTrips += len(trips_sample)\n",
    "                    oTimes += trips_sample['queryTime'].sum()\n",
    "\n",
    "                kResAppend = {}\n",
    "                kResAppend['oa_id'] = orow['oa_id']\n",
    "                kResAppend['score'] = statistics.mean(oCosts)\n",
    "                kResAppend['times'] = oTimes\n",
    "                kResList.append(kResAppend)\n",
    "\n",
    "            knnres = pd.DataFrame(kResList).set_index('oa_id')\n",
    "\n",
    "            peformance_mx['knn_{}'.format(k)] = knnres['score']\n",
    "            costing_mx['KNN_{}'.format(k)] = knnres['times']\n",
    "            computing_times['KNN_{}'.format(k)] = processing_time\n",
    "\n",
    "        print('process k mean clustering')\n",
    "        #Run k-means\n",
    "\n",
    "        for num_clusters in [3,5,7,9]:\n",
    "\n",
    "            processing_time = 0\n",
    "\n",
    "            t0 = time.time()\n",
    "            kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=\"auto\").fit(POISample[['poi_lon','poi_lat']].values)\n",
    "            # Get cluster labels and centroids\n",
    "            cluster_labels = kmeans.labels_\n",
    "            t1 = time.time()\n",
    "            processing_time += (t1 - t0)\n",
    "\n",
    "            POISample['cluster'] = cluster_labels\n",
    "\n",
    "            flows_df_list = []\n",
    "\n",
    "            for cluster in set(cluster_labels):\n",
    "                t0 = time.time()\n",
    "                points_in_cluster = np.where(cluster_labels == cluster)[0]\n",
    "                centroid = [POISample[POISample['cluster'] == cluster]['poi_lon'].mean(),POISample[POISample['cluster'] == cluster]['poi_lat'].mean()]\n",
    "                distances_to_centroid = np.linalg.norm(POISample[['poi_lon','poi_lat']].values[points_in_cluster] - POISample[['poi_lon','poi_lat']].values[points_in_cluster].mean(axis = 0), axis=1)\n",
    "                # Find the index of the point closest to the centroid\n",
    "                closest_point_index = points_in_cluster[np.argmin(distances_to_centroid)]\n",
    "                flow_id = POISample.iloc[closest_point_index]['poi_id']\n",
    "                t1 = time.time()\n",
    "                processing_time += (t1 - t0)\n",
    "                oa_count = 0\n",
    "                for oa in list(oaSample['oa_id']):\n",
    "                    oa_count += 1\n",
    "                    flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                    query_times = list(flow_gac['queryTime'])\n",
    "\n",
    "                    for poi in points_in_cluster:\n",
    "                        flow_append = flow_gac.copy()\n",
    "                        if poi != closest_point_index:\n",
    "                            flow_append['queryTime'] = 0\n",
    "                        else:\n",
    "                            flow_append['queryTime'] = query_times\n",
    "                        flow_append['poi_id'] = POISample.iloc[poi]['poi_id']\n",
    "                        flows_df_list.append(flow_append)\n",
    "\n",
    "            flows_df = pd.concat(flows_df_list, ignore_index=True)\n",
    "            flows_df['att'] = flows_df['poi_id'].map(attractivnessDict)\n",
    "            flows_df['dist decay'] = vfunc(np.array(flows_df['gac']),decay_constant, exponent)\n",
    "            flows_df['grav'] = flows_df['dist decay'] * flows_df['att']\n",
    "            peformance_mx['kmean_{}'.format(num_clusters)] = flows_df.groupby('oa_id').sum()['grav']\n",
    "            costing_mx['kmean_{}'.format(num_clusters)] = flows_df.groupby('oa_id').sum()['queryTime']\n",
    "            computing_times['kmean_{}'.format(num_clusters)] = processing_time\n",
    "\n",
    "        print('process dbscan clustering')\n",
    "        #DBSCAN\n",
    "        eps_test = [0.01,0.02,0.03]\n",
    "        min_samples = [1,3,5]\n",
    "\n",
    "        for ep in eps_test:\n",
    "            for ms in min_samples:\n",
    "                processing_time = 0\n",
    "\n",
    "                t0 = time.time()\n",
    "                dbscan = DBSCAN(eps=ep, min_samples=ms).fit(POISample[['poi_lon','poi_lat']].values)\n",
    "\n",
    "                # Get cluster labels and centroids\n",
    "                cluster_labels = dbscan.labels_\n",
    "                POISample['cluster'] = cluster_labels\n",
    "                t1 = time.time()\n",
    "                processing_time += (t1 - t0)\n",
    "                flows_df_list = []\n",
    "\n",
    "                for cluster in set(cluster_labels):\n",
    "                    if cluster == -1:\n",
    "                        for c_ind in np.where(cluster_labels == cluster)[0]:\n",
    "                            flow_id = POISample.iloc[c_ind]['poi_id']\n",
    "\n",
    "                            for oa in list(oaSample['oa_id']):\n",
    "                                flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                                flows_df_list.append(flow_gac)\n",
    "                    else:\n",
    "                        t0 = time.time()\n",
    "                        points_in_cluster = np.where(cluster_labels == cluster)[0]\n",
    "                        centroid = [POISample[POISample['cluster'] == cluster]['poi_lon'].mean(),POISample[POISample['cluster'] == cluster]['poi_lat'].mean()]\n",
    "                        distances_to_centroid = np.linalg.norm(POISample[['poi_lon','poi_lat']].values[points_in_cluster] - POISample[['poi_lon','poi_lat']].values[points_in_cluster].mean(axis = 0), axis=1)\n",
    "                        # Find the index of the point closest to the centroid\n",
    "                        closest_point_index = points_in_cluster[np.argmin(distances_to_centroid)]\n",
    "                        flow_id = POISample.iloc[closest_point_index]['poi_id']\n",
    "                        t1 = time.time()\n",
    "                        processing_time += (t1 - t0)\n",
    "                        for oa in list(oaSample['oa_id']):\n",
    "                            flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                            query_times = list(flow_gac['queryTime'])\n",
    "                            for poi in points_in_cluster:\n",
    "                                flow_append = flow_gac.copy()\n",
    "                                if poi != closest_point_index:\n",
    "                                    flow_append['queryTime'] = 0\n",
    "                                else:\n",
    "                                    flow_append['queryTime'] = query_times\n",
    "                                flow_append['poi_id'] = POISample.iloc[poi]['poi_id']\n",
    "                                flows_df_list.append(flow_append)\n",
    "\n",
    "                flows_df = pd.concat(flows_df_list, ignore_index=True)\n",
    "                flows_df['att'] = flows_df['poi_id'].map(attractivnessDict)\n",
    "                flows_df['dist decay'] = vfunc(np.array(flows_df['gac']),decay_constant, exponent)\n",
    "                flows_df['grav'] = flows_df['dist decay'] * flows_df['att']\n",
    "                peformance_mx['dbscan_{}_{}'.format(ep,ms)] = flows_df.groupby('oa_id').sum()['grav']\n",
    "                costing_mx['dbscan_{}_{}'.format(ep,ms)] = flows_df.groupby('oa_id').sum()['queryTime']\n",
    "                computing_times['dbscan_{}_{}'.format(ep,ms)] = processing_time\n",
    "\n",
    "        print('process hdbscan clustering')\n",
    "        #HDBSCAN\n",
    "        min_clusters = [3,5,7]\n",
    "        min_sample_tests = [1,3,5,7]\n",
    "\n",
    "        for mc in min_clusters:\n",
    "            for ms in min_sample_tests:\n",
    "\n",
    "                processing_time = 0\n",
    "\n",
    "                t0 = time.time()\n",
    "                hdbscan = HDBSCAN(min_cluster_size = mc, min_samples=ms).fit(POISample[['poi_lon','poi_lat']].values)\n",
    "\n",
    "                # Get cluster labels and centroids\n",
    "                cluster_labels = hdbscan.labels_\n",
    "                POISample['cluster'] = cluster_labels\n",
    "                t1 = time.time()\n",
    "                processing_time += (t1 - t0)\n",
    "\n",
    "                flows_df_list = []\n",
    "\n",
    "                for cluster in set(cluster_labels):\n",
    "                    if cluster == -1:\n",
    "                        for c_ind in np.where(cluster_labels == cluster)[0]:\n",
    "                            flow_id = POISample.iloc[c_ind]['poi_id']\n",
    "\n",
    "                            for oa in list(oaSample['oa_id']):\n",
    "                                flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                                flows_df_list.append(flow_gac)\n",
    "                    else:\n",
    "                        t0 = time.time()\n",
    "                        points_in_cluster = np.where(cluster_labels == cluster)[0]\n",
    "                        centroid = [POISample[POISample['cluster'] == cluster]['poi_lon'].mean(),POISample[POISample['cluster'] == cluster]['poi_lat'].mean()]\n",
    "                        distances_to_centroid = np.linalg.norm(POISample[['poi_lon','poi_lat']].values[points_in_cluster] - POISample[['poi_lon','poi_lat']].values[points_in_cluster].mean(axis = 0), axis=1)\n",
    "                        # Find the index of the point closest to the centroid\n",
    "                        closest_point_index = points_in_cluster[np.argmin(distances_to_centroid)]\n",
    "                        flow_id = POISample.iloc[closest_point_index]['poi_id']\n",
    "                        t1 = time.time()\n",
    "                        processing_time += (t1 - t0)\n",
    "                        for oa in list(oaSample['oa_id']):\n",
    "                            flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                            query_times = list(flow_gac['queryTime'])\n",
    "                            for poi in points_in_cluster:\n",
    "                                flow_append = flow_gac.copy()\n",
    "                                if poi != closest_point_index:\n",
    "                                    flow_append['queryTime'] = 0\n",
    "                                else:\n",
    "                                    flow_append['queryTime'] = query_times\n",
    "                                flow_append['poi_id'] = POISample.iloc[poi]['poi_id']\n",
    "                                flows_df_list.append(flow_append)\n",
    "\n",
    "                flows_df = pd.concat(flows_df_list, ignore_index=True)\n",
    "                flows_df['att'] = flows_df['poi_id'].map(attractivnessDict)\n",
    "                flows_df['dist decay'] = vfunc(np.array(flows_df['gac']),decay_constant, exponent)\n",
    "                flows_df['grav'] = flows_df['dist decay'] * flows_df['att']\n",
    "                peformance_mx['hdbscan_{}_{}'.format(mc,ms)] = flows_df.groupby('oa_id').sum()['grav']\n",
    "                costing_mx['hdbscan_{}_{}'.format(mc,ms)] = flows_df.groupby('oa_id').sum()['queryTime']\n",
    "                computing_times['hdbscan_{}_{}'.format(mc,ms)] = processing_time\n",
    "\n",
    "\n",
    "        # Get flow\n",
    "        # Cluster flows\n",
    "\n",
    "        print('process flow clustering')\n",
    "\n",
    "        minflows = [5,7]\n",
    "        minclusters = [5,15]\n",
    "        minsamples = [5,15]\n",
    "\n",
    "        for mf in minflows:\n",
    "            t0 = time.time()\n",
    "            X, o_index, d_index, flows, flows_index = get_flow_dist_mx(oaSample,POISample,mf,flowProx)\n",
    "            t1 = time.time()\n",
    "            get_flows_time = t1 - t0\n",
    "            for mc in minclusters:\n",
    "                for ms in minsamples:\n",
    "                    \n",
    "                    processing_time = 0\n",
    "\n",
    "                    t0 = time.time()\n",
    "                    hdb = HDBSCAN(min_cluster_size=mc, min_samples=ms, metric=getminreach).fit(X)\n",
    "                    cluster_labels = hdb.labels_\n",
    "                    t1 = time.time()\n",
    "                    processing_time += (t1 - t0)\n",
    "                    flows_df_list = []\n",
    "                    for cluster in set(cluster_labels):\n",
    "                        if cluster == -1:\n",
    "                            cluster_indeces = np.where(cluster_labels==cluster)[0]\n",
    "                            flow_inds = list(set([flows_index[i] for i in cluster_indeces]))\n",
    "                            for f in flow_inds:\n",
    "                                flow_gac = trips.loc[oa_poi_id_dict[f[0]][f[1]]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                                flows_df_list.append(flow_gac)\n",
    "                        else:\n",
    "                            t0 = time.time()\n",
    "                            cluster_indeces = np.where(cluster_labels==cluster)[0]\n",
    "                            #Select origin of best flow\n",
    "                            oas_in_flow = list(set([o_index[i] for i in cluster_indeces]))\n",
    "                            distances_to_centroid = np.linalg.norm(oaSample.set_index('oa_id').loc[oas_in_flow].values - oaSample.set_index('oa_id').loc[oas_in_flow].values.mean(axis = 0),axis=1)\n",
    "                            flow_oa = oas_in_flow[np.argmin(distances_to_centroid)]\n",
    "                            #Select destination of best flow\n",
    "                            poi_ids = list(set([d_index[i] for i in cluster_indeces]))\n",
    "                            distances_to_centroid = np.linalg.norm(POISample.set_index('poi_id').loc[poi_ids][['poi_lon','poi_lat']].values - POISample.set_index('poi_id').loc[poi_ids][['poi_lon','poi_lat']].values.mean(axis = 0),axis = 1)\n",
    "                            flow_poi = poi_ids[np.argmin(distances_to_centroid)]\n",
    "                            t1 = time.time()\n",
    "                            processing_time += (t1 - t0)\n",
    "                            #measure GAC for all time steps\n",
    "                            flow_gac = trips.loc[oa_poi_id_dict[flow_oa][flow_poi]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                            query_times = list(flow_gac['queryTime'])\n",
    "                            flow_inds = list(set([flows_index[i] for i in cluster_indeces]))\n",
    "                            for f in flow_inds:\n",
    "                                flow_append = flow_gac.copy()\n",
    "                                if f[1] != flow_poi:\n",
    "                                    flow_append['queryTime'] = 0\n",
    "                                else:\n",
    "                                    flow_append['queryTime'] = query_times\n",
    "                                flow_append['oa_id'] = f[0]\n",
    "                                flow_append['poi_id'] = f[1]\n",
    "                                flows_df_list.append(flow_append)\n",
    "                    flows_df = pd.concat(flows_df_list, ignore_index=True)\n",
    "                    flows_df['att'] = flows_df['poi_id'].map(attractivnessDict)\n",
    "                    flows_df['dist decay'] = vfunc(np.array(flows_df['gac']),decay_constant, exponent)\n",
    "                    flows_df['grav'] = flows_df['dist decay'] * flows_df['att']\n",
    "                    peformance_mx['flowhdbscan_{}_{}_{}'.format(mf,mc,ms)] = flows_df.groupby('oa_id').sum()['grav']\n",
    "                    costing_mx['flowhdbscan_{}_{}_{}'.format(mf,mc,ms)] = flows_df.groupby('oa_id').sum()['queryTime']\n",
    "                    computing_times['flowhdbscan_{}_{}_{}'.format(mf,mc,ms)] = processing_time\n",
    "                    computing_times['get_flow_flowhdbscan_{}_{}_{}'.format(mf,mc,ms)] = get_flows_time\n",
    "\n",
    "\n",
    "        print('process gravity trip generator')\n",
    "\n",
    "        # Gravit Trip Generator\n",
    "        distMxList = []\n",
    "\n",
    "        for i,r in oaSample.iterrows():\n",
    "            for i_, r_ in POISample.iterrows():\n",
    "                rowAppend = {}\n",
    "                rowAppend['oa'] = r['oa_id']\n",
    "                rowAppend['poi'] = r_['poi_id']\n",
    "                rowAppend['dist'] = haversine_distance(r['oa_lon'], r['oa_lat'], r_['poi_lon'], r_['poi_lat'])\n",
    "                distMxList.append(rowAppend)\n",
    "\n",
    "        distMx = pd.DataFrame(distMxList)\n",
    "        distMx['att'] = distMx['poi'].map(attractivnessDict)\n",
    "\n",
    "        distsDecay = []\n",
    "        for i in np.array(distMx['dist']):\n",
    "            distsDecay.append(distance_decay(i, decay_constant, exponent))\n",
    "\n",
    "        distMx['decay'] = distsDecay\n",
    "        distMx['grav'] = distMx['decay'] * distMx['att']\n",
    "        distMx['gravN'] = (distMx['grav'] - distMx['grav'].min()) / (distMx['grav'].max() - distMx['grav'].min())\n",
    "\n",
    "        distMx = distMx.merge(trips.groupby(['oa_id','poi_id']).count()['departure_time'].rename('tripCount'), left_on = ['oa','poi'],right_index = True)\n",
    "        distMx['tripsSample'] = distMx['tripCount'] * distMx['gravN']\n",
    "\n",
    "        resultsList = []\n",
    "        countOrigins = 0\n",
    "\n",
    "        for o in list(oaSample['oa_id']):\n",
    "            countOrigins += 1\n",
    "            oCosts = []\n",
    "            oTrips = 0\n",
    "            oTimes = 0\n",
    "            for p in list(POISample['poi_id']):\n",
    "                if len(oa_poi_id_dict[o][p]) > 0:\n",
    "                    numSample = int(distMx[(distMx['oa'] == o) & (distMx['poi'] == p)]['tripsSample'].values[0])\n",
    "                    tripSample = trips.loc[oa_poi_id_dict[o][p]].sample(numSample)\n",
    "                    oCosts = oCosts + list(tripSample['gac'])\n",
    "                    oTrips += numSample\n",
    "                    oTimes += tripSample['queryTime'].sum()\n",
    "            rowAppend = {}\n",
    "            rowAppend['OA'] = o\n",
    "            rowAppend['GTG'] = statistics.mean(oCosts)\n",
    "            rowAppend['GTG_Trips'] = oTrips\n",
    "            rowAppend['GTG_Times'] = oTimes\n",
    "            resultsList.append(rowAppend)\n",
    "\n",
    "        tripGenGrav = pd.DataFrame(resultsList).set_index('OA')\n",
    "\n",
    "        peformance_mx['g-tgm'] = tripGenGrav['GTG']\n",
    "        costing_mx['g-tgm'] = tripGenGrav['GTG_Times']\n",
    "\n",
    "        performance[it][stratum] = peformance_mx\n",
    "        processing_times[it][stratum] = costing_mx\n",
    "        exp_meta_data[it][stratum]['processing_times'] = computing_times\n",
    "\n",
    "        with open('results/performance.pkl', 'wb') as f:\n",
    "            pickle.dump(performance, f)\n",
    "\n",
    "        with open('results/processing_times.pkl', 'wb') as f:\n",
    "            pickle.dump(processing_times, f)\n",
    "\n",
    "        with open('results/exp_meta_data.pkl', 'wb') as f:\n",
    "            pickle.dump(exp_meta_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for stratum in stratumDict.keys():\n",
    "stratum = list(stratumDict.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8264/4110636532.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['attractiveness'] = POISample['poi_id'].map(attractivnessDict)\n"
     ]
    }
   ],
   "source": [
    "initial_trips = pd.read_csv('results/trips_samples/trips_to_route_{}_{}.csv'.format(it,stratum)).set_index('trip_id')\n",
    "trips = pd.read_csv('results/trips_samples/results_full_{}_{}.csv'.format(it,stratum)).set_index('trip_id')\n",
    "trips = trips.merge(initial_trips[['poi_id','oa_id','time']],right_index=True, left_index=True)\n",
    "\n",
    "#Sample 200 random zones\n",
    "oaSample = oa_info[oa_info['oa_id'].isin(list(set(trips['oa_id'])))][['oa_id','oa_lat','oa_lon']]\n",
    "\n",
    "#POIs\n",
    "POISample = pois[pois['poi_id'].isin(list(set(trips['poi_id'])))]\n",
    "POISample['attractiveness'] = POISample['poi_id'].map(attractivnessDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process knn results\n",
      "process k mean clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8264/1827552255.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process dbscan clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8264/1827552255.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process hdbscan clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8264/1827552255.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n",
      "/tmp/ipykernel_8264/1827552255.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  POISample['cluster'] = cluster_labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process flow clustering\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 264\u001b[0m\n\u001b[1;32m    261\u001b[0m processing_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    263\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 264\u001b[0m hdb \u001b[38;5;241m=\u001b[39m \u001b[43mHDBSCAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_cluster_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgetminreach\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m cluster_labels \u001b[38;5;241m=\u001b[39m hdb\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[1;32m    266\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/envs/access/lib/python3.12/site-packages/sklearn/cluster/_hdbscan/hdbscan.py:819\u001b[0m, in \u001b[0;36mHDBSCAN.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    816\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgo\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mball_tree\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleaf_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleaf_size\n\u001b[0;32m--> 819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_linkage_tree_ \u001b[38;5;241m=\u001b[39m \u001b[43mmst_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobabilities_ \u001b[38;5;241m=\u001b[39m tree_to_labels(\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_linkage_tree_,\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_cluster_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_cluster_size,\n\u001b[1;32m    828\u001b[0m )\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_finite:\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;66;03m# Remap indices to align with original data in the case of\u001b[39;00m\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;66;03m# non-finite entries. Samples with np.inf are mapped to -1 and\u001b[39;00m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;66;03m# those with np.nan are mapped to -2.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/access/lib/python3.12/site-packages/sklearn/cluster/_hdbscan/hdbscan.py:250\u001b[0m, in \u001b[0;36m_hdbscan_brute\u001b[0;34m(X, min_samples, alpha, metric, n_jobs, copy, **metric_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m     distance_matrix \u001b[38;5;241m=\u001b[39m distance_matrix\u001b[38;5;241m.\u001b[39mtocsr()\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Note that `distance_matrix` is manipulated in-place, however we do not\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# need it for anything else past this point, hence the operation is safe.\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m mutual_reachability_ \u001b[38;5;241m=\u001b[39m \u001b[43mmutual_reachability_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_distance\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m min_spanning_tree \u001b[38;5;241m=\u001b[39m _brute_mst(mutual_reachability_, min_samples\u001b[38;5;241m=\u001b[39mmin_samples)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Warn if the MST couldn't be constructed around the missing distances\u001b[39;00m\n",
      "File \u001b[0;32msklearn/cluster/_hdbscan/_reachability.pyx:102\u001b[0m, in \u001b[0;36msklearn.cluster._hdbscan._reachability.mutual_reachability_graph\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/cluster/_hdbscan/_reachability.pyx:134\u001b[0m, in \u001b[0;36msklearn.cluster._hdbscan._reachability._dense_mutual_reachability_graph\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/access/lib/python3.12/site-packages/numpy/core/fromnumeric.py:770\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[1;32m    768\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 770\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m a\u001b[38;5;241m.\u001b[39mpartition(kth, axis\u001b[38;5;241m=\u001b[39maxis, kind\u001b[38;5;241m=\u001b[39mkind, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exp_meta_data[it][stratum]['num_trips_initial'] = len(initial_trips)\n",
    "exp_meta_data[it][stratum]['num_trips_costed'] = len(trips)\n",
    "\n",
    "#OA-POI Index Dict\n",
    "oa_poi_id_dict = {}\n",
    "\n",
    "for oa_id in list(oaSample['oa_id']):\n",
    "    oa_poi_id_dict[oa_id] = {}\n",
    "    for poi_id in list(POISample['poi_id']):\n",
    "        oa_poi_id_dict[oa_id][poi_id] = list(trips[(trips['oa_id'] == oa_id) & (trips['poi_id'] == poi_id)].index)\n",
    "\n",
    "peformance_mx = pd.DataFrame(index = list(oaSample['oa_id']))\n",
    "costing_mx = pd.DataFrame(index = list(oaSample['oa_id']))\n",
    "computing_times = {}\n",
    "\n",
    "#Compute generalised access cost\n",
    "trips['gac'] = (( 1.5 * (trips['total_time'])) - (0.5 * trips['transit_time']) + ((trips['fare'] * 3600) / 6.7) + (10 * trips['num_transfers'])) / 60\n",
    "trips['att'] = trips['poi_id'].map(attractivnessDict)\n",
    "\n",
    "#Gravity Model Ground Truth\n",
    "trips['dist decay'] = vfunc(np.array(trips['gac']),decay_constant, exponent)\n",
    "trips['grav'] = trips['dist decay'] * trips['att']\n",
    "gravity = trips.groupby('oa_id').sum()['grav']\n",
    "\n",
    "peformance_mx['GM'] = gravity\n",
    "costing_mx['GM'] = trips.groupby('oa_id').sum()['queryTime']\n",
    "\n",
    "# Run KNN\n",
    "\n",
    "print('process knn results')\n",
    "\n",
    "for k in [5,15,25]:\n",
    "\n",
    "    processing_time = 0\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    knn.fit(POISample[['poi_lon','poi_lat']].values)\n",
    "    kResList = []\n",
    "    countOrigins = 0\n",
    "\n",
    "    t1 = time.time()\n",
    "    processing_time += (t1 - t0)\n",
    "\n",
    "    for oind, orow in oaSample.iterrows():\n",
    "        t0 = time.time()\n",
    "        distances, indices = knn.kneighbors(orow[['oa_lon','oa_lat']].values.reshape(1, -1))\n",
    "        t1 = time.time()\n",
    "        processing_time += (t1 - t0)\n",
    "        \n",
    "        oCosts = []\n",
    "        oTrips = 0\n",
    "        oTimes = 0\n",
    "\n",
    "        for i in indices[0]:\n",
    "            pid = POISample.iloc[i]['poi_id']\n",
    "            #trips_sample = trips[(trips['oa_id'] == orow['oa_id']) & (trips['poi_id'] == pid)]\n",
    "            trips_sample = trips.loc[oa_poi_id_dict[orow['oa_id']][pid]]\n",
    "            oCosts = oCosts + list(trips_sample['gac'])\n",
    "            oTrips += len(trips_sample)\n",
    "            oTimes += trips_sample['queryTime'].sum()\n",
    "\n",
    "        kResAppend = {}\n",
    "        kResAppend['oa_id'] = orow['oa_id']\n",
    "        kResAppend['score'] = statistics.mean(oCosts)\n",
    "        kResAppend['times'] = oTimes\n",
    "        kResList.append(kResAppend)\n",
    "\n",
    "    knnres = pd.DataFrame(kResList).set_index('oa_id')\n",
    "\n",
    "    peformance_mx['knn_{}'.format(k)] = knnres['score']\n",
    "    costing_mx['KNN_{}'.format(k)] = knnres['times']\n",
    "    computing_times['KNN_{}'.format(k)] = processing_time\n",
    "\n",
    "print('process k mean clustering')\n",
    "#Run k-means\n",
    "\n",
    "for num_clusters in [3,5,7,9]:\n",
    "\n",
    "    processing_time = 0\n",
    "\n",
    "    t0 = time.time()\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=\"auto\").fit(POISample[['poi_lon','poi_lat']].values)\n",
    "    # Get cluster labels and centroids\n",
    "    cluster_labels = kmeans.labels_\n",
    "    t1 = time.time()\n",
    "    processing_time += (t1 - t0)\n",
    "\n",
    "    POISample['cluster'] = cluster_labels\n",
    "\n",
    "    flows_df_list = []\n",
    "\n",
    "    for cluster in set(cluster_labels):\n",
    "        t0 = time.time()\n",
    "        points_in_cluster = np.where(cluster_labels == cluster)[0]\n",
    "        centroid = [POISample[POISample['cluster'] == cluster]['poi_lon'].mean(),POISample[POISample['cluster'] == cluster]['poi_lat'].mean()]\n",
    "        distances_to_centroid = np.linalg.norm(POISample[['poi_lon','poi_lat']].values[points_in_cluster] - POISample[['poi_lon','poi_lat']].values[points_in_cluster].mean(axis = 0), axis=1)\n",
    "        # Find the index of the point closest to the centroid\n",
    "        closest_point_index = points_in_cluster[np.argmin(distances_to_centroid)]\n",
    "        flow_id = POISample.iloc[closest_point_index]['poi_id']\n",
    "        t1 = time.time()\n",
    "        processing_time += (t1 - t0)\n",
    "        oa_count = 0\n",
    "        for oa in list(oaSample['oa_id']):\n",
    "            oa_count += 1\n",
    "            flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "            query_times = list(flow_gac['queryTime'])\n",
    "\n",
    "            for poi in points_in_cluster:\n",
    "                flow_append = flow_gac.copy()\n",
    "                if poi != closest_point_index:\n",
    "                    flow_append['queryTime'] = 0\n",
    "                else:\n",
    "                    flow_append['queryTime'] = query_times\n",
    "                flow_append['poi_id'] = POISample.iloc[poi]['poi_id']\n",
    "                flows_df_list.append(flow_append)\n",
    "\n",
    "    flows_df = pd.concat(flows_df_list, ignore_index=True)\n",
    "    flows_df['att'] = flows_df['poi_id'].map(attractivnessDict)\n",
    "    flows_df['dist decay'] = vfunc(np.array(flows_df['gac']),decay_constant, exponent)\n",
    "    flows_df['grav'] = flows_df['dist decay'] * flows_df['att']\n",
    "    peformance_mx['kmean_{}'.format(num_clusters)] = flows_df.groupby('oa_id').sum()['grav']\n",
    "    costing_mx['kmean_{}'.format(num_clusters)] = flows_df.groupby('oa_id').sum()['queryTime']\n",
    "    computing_times['kmean_{}'.format(num_clusters)] = processing_time\n",
    "\n",
    "print('process dbscan clustering')\n",
    "#DBSCAN\n",
    "eps_test = [0.01,0.02,0.03]\n",
    "min_samples = [1,3,5]\n",
    "\n",
    "for ep in eps_test:\n",
    "    for ms in min_samples:\n",
    "        processing_time = 0\n",
    "\n",
    "        t0 = time.time()\n",
    "        dbscan = DBSCAN(eps=ep, min_samples=ms).fit(POISample[['poi_lon','poi_lat']].values)\n",
    "\n",
    "        # Get cluster labels and centroids\n",
    "        cluster_labels = dbscan.labels_\n",
    "        POISample['cluster'] = cluster_labels\n",
    "        t1 = time.time()\n",
    "        processing_time += (t1 - t0)\n",
    "        flows_df_list = []\n",
    "\n",
    "        for cluster in set(cluster_labels):\n",
    "            if cluster == -1:\n",
    "                for c_ind in np.where(cluster_labels == cluster)[0]:\n",
    "                    flow_id = POISample.iloc[c_ind]['poi_id']\n",
    "\n",
    "                    for oa in list(oaSample['oa_id']):\n",
    "                        flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                        flows_df_list.append(flow_gac)\n",
    "            else:\n",
    "                t0 = time.time()\n",
    "                points_in_cluster = np.where(cluster_labels == cluster)[0]\n",
    "                centroid = [POISample[POISample['cluster'] == cluster]['poi_lon'].mean(),POISample[POISample['cluster'] == cluster]['poi_lat'].mean()]\n",
    "                distances_to_centroid = np.linalg.norm(POISample[['poi_lon','poi_lat']].values[points_in_cluster] - POISample[['poi_lon','poi_lat']].values[points_in_cluster].mean(axis = 0), axis=1)\n",
    "                # Find the index of the point closest to the centroid\n",
    "                closest_point_index = points_in_cluster[np.argmin(distances_to_centroid)]\n",
    "                flow_id = POISample.iloc[closest_point_index]['poi_id']\n",
    "                t1 = time.time()\n",
    "                processing_time += (t1 - t0)\n",
    "                for oa in list(oaSample['oa_id']):\n",
    "                    flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                    query_times = list(flow_gac['queryTime'])\n",
    "                    for poi in points_in_cluster:\n",
    "                        flow_append = flow_gac.copy()\n",
    "                        if poi != closest_point_index:\n",
    "                            flow_append['queryTime'] = 0\n",
    "                        else:\n",
    "                            flow_append['queryTime'] = query_times\n",
    "                        flow_append['poi_id'] = POISample.iloc[poi]['poi_id']\n",
    "                        flows_df_list.append(flow_append)\n",
    "\n",
    "        flows_df = pd.concat(flows_df_list, ignore_index=True)\n",
    "        flows_df['att'] = flows_df['poi_id'].map(attractivnessDict)\n",
    "        flows_df['dist decay'] = vfunc(np.array(flows_df['gac']),decay_constant, exponent)\n",
    "        flows_df['grav'] = flows_df['dist decay'] * flows_df['att']\n",
    "        peformance_mx['dbscan_{}_{}'.format(ep,ms)] = flows_df.groupby('oa_id').sum()['grav']\n",
    "        costing_mx['dbscan_{}_{}'.format(ep,ms)] = flows_df.groupby('oa_id').sum()['queryTime']\n",
    "        computing_times['dbscan_{}_{}'.format(ep,ms)] = processing_time\n",
    "\n",
    "print('process hdbscan clustering')\n",
    "#HDBSCAN\n",
    "min_clusters = [3,5,7]\n",
    "min_sample_tests = [1,3,5,7]\n",
    "\n",
    "for mc in min_clusters:\n",
    "    for ms in min_sample_tests:\n",
    "\n",
    "        processing_time = 0\n",
    "\n",
    "        t0 = time.time()\n",
    "        hdbscan = HDBSCAN(min_cluster_size = mc, min_samples=ms).fit(POISample[['poi_lon','poi_lat']].values)\n",
    "\n",
    "        # Get cluster labels and centroids\n",
    "        cluster_labels = hdbscan.labels_\n",
    "        POISample['cluster'] = cluster_labels\n",
    "        t1 = time.time()\n",
    "        processing_time += (t1 - t0)\n",
    "\n",
    "        flows_df_list = []\n",
    "\n",
    "        for cluster in set(cluster_labels):\n",
    "            if cluster == -1:\n",
    "                for c_ind in np.where(cluster_labels == cluster)[0]:\n",
    "                    flow_id = POISample.iloc[c_ind]['poi_id']\n",
    "\n",
    "                    for oa in list(oaSample['oa_id']):\n",
    "                        flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                        flows_df_list.append(flow_gac)\n",
    "            else:\n",
    "                t0 = time.time()\n",
    "                points_in_cluster = np.where(cluster_labels == cluster)[0]\n",
    "                centroid = [POISample[POISample['cluster'] == cluster]['poi_lon'].mean(),POISample[POISample['cluster'] == cluster]['poi_lat'].mean()]\n",
    "                distances_to_centroid = np.linalg.norm(POISample[['poi_lon','poi_lat']].values[points_in_cluster] - POISample[['poi_lon','poi_lat']].values[points_in_cluster].mean(axis = 0), axis=1)\n",
    "                # Find the index of the point closest to the centroid\n",
    "                closest_point_index = points_in_cluster[np.argmin(distances_to_centroid)]\n",
    "                flow_id = POISample.iloc[closest_point_index]['poi_id']\n",
    "                t1 = time.time()\n",
    "                processing_time += (t1 - t0)\n",
    "                for oa in list(oaSample['oa_id']):\n",
    "                    flow_gac = trips.loc[oa_poi_id_dict[oa][flow_id]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                    query_times = list(flow_gac['queryTime'])\n",
    "                    for poi in points_in_cluster:\n",
    "                        flow_append = flow_gac.copy()\n",
    "                        if poi != closest_point_index:\n",
    "                            flow_append['queryTime'] = 0\n",
    "                        else:\n",
    "                            flow_append['queryTime'] = query_times\n",
    "                        flow_append['poi_id'] = POISample.iloc[poi]['poi_id']\n",
    "                        flows_df_list.append(flow_append)\n",
    "\n",
    "        flows_df = pd.concat(flows_df_list, ignore_index=True)\n",
    "        flows_df['att'] = flows_df['poi_id'].map(attractivnessDict)\n",
    "        flows_df['dist decay'] = vfunc(np.array(flows_df['gac']),decay_constant, exponent)\n",
    "        flows_df['grav'] = flows_df['dist decay'] * flows_df['att']\n",
    "        peformance_mx['hdbscan_{}_{}'.format(mc,ms)] = flows_df.groupby('oa_id').sum()['grav']\n",
    "        costing_mx['hdbscan_{}_{}'.format(mc,ms)] = flows_df.groupby('oa_id').sum()['queryTime']\n",
    "        computing_times['hdbscan_{}_{}'.format(mc,ms)] = processing_time\n",
    "\n",
    "\n",
    "# Get flow\n",
    "# Cluster flows\n",
    "\n",
    "print('process flow clustering')\n",
    "\n",
    "minflows = [5,7]\n",
    "minclusters = [5,15]\n",
    "minsamples = [5,15]\n",
    "\n",
    "for mf in minflows:\n",
    "    t0 = time.time()\n",
    "    X, o_index, d_index, flows, flows_index = get_flow_dist_mx(oaSample,POISample,mf,flowProx)\n",
    "    t1 = time.time()\n",
    "    get_flows_time = t1 - t0\n",
    "    for mc in minclusters:\n",
    "        for ms in minsamples:\n",
    "            \n",
    "            processing_time = 0\n",
    "\n",
    "            t0 = time.time()\n",
    "            hdb = HDBSCAN(min_cluster_size=mc, min_samples=ms, metric=getminreach).fit(X)\n",
    "            cluster_labels = hdb.labels_\n",
    "            t1 = time.time()\n",
    "            processing_time += (t1 - t0)\n",
    "            flows_df_list = []\n",
    "            for cluster in set(cluster_labels):\n",
    "                if cluster == -1:\n",
    "                    cluster_indeces = np.where(cluster_labels==cluster)[0]\n",
    "                    flow_inds = list(set([flows_index[i] for i in cluster_indeces]))\n",
    "                    for f in flow_inds:\n",
    "                        flow_gac = trips.loc[oa_poi_id_dict[f[0]][f[1]]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                        flows_df_list.append(flow_gac)\n",
    "                else:\n",
    "                    t0 = time.time()\n",
    "                    cluster_indeces = np.where(cluster_labels==cluster)[0]\n",
    "                    #Select origin of best flow\n",
    "                    oas_in_flow = list(set([o_index[i] for i in cluster_indeces]))\n",
    "                    distances_to_centroid = np.linalg.norm(oaSample.set_index('oa_id').loc[oas_in_flow].values - oaSample.set_index('oa_id').loc[oas_in_flow].values.mean(axis = 0),axis=1)\n",
    "                    flow_oa = oas_in_flow[np.argmin(distances_to_centroid)]\n",
    "                    #Select destination of best flow\n",
    "                    poi_ids = list(set([d_index[i] for i in cluster_indeces]))\n",
    "                    distances_to_centroid = np.linalg.norm(POISample.set_index('poi_id').loc[poi_ids][['poi_lon','poi_lat']].values - POISample.set_index('poi_id').loc[poi_ids][['poi_lon','poi_lat']].values.mean(axis = 0),axis = 1)\n",
    "                    flow_poi = poi_ids[np.argmin(distances_to_centroid)]\n",
    "                    t1 = time.time()\n",
    "                    processing_time += (t1 - t0)\n",
    "                    #measure GAC for all time steps\n",
    "                    flow_gac = trips.loc[oa_poi_id_dict[flow_oa][flow_poi]][['oa_id','poi_id','time','gac','queryTime']]\n",
    "                    query_times = list(flow_gac['queryTime'])\n",
    "                    flow_inds = list(set([flows_index[i] for i in cluster_indeces]))\n",
    "                    for f in flow_inds:\n",
    "                        flow_append = flow_gac.copy()\n",
    "                        if f[1] != flow_poi:\n",
    "                            flow_append['queryTime'] = 0\n",
    "                        else:\n",
    "                            flow_append['queryTime'] = query_times\n",
    "                        flow_append['oa_id'] = f[0]\n",
    "                        flow_append['poi_id'] = f[1]\n",
    "                        flows_df_list.append(flow_append)\n",
    "            flows_df = pd.concat(flows_df_list, ignore_index=True)\n",
    "            flows_df['att'] = flows_df['poi_id'].map(attractivnessDict)\n",
    "            flows_df['dist decay'] = vfunc(np.array(flows_df['gac']),decay_constant, exponent)\n",
    "            flows_df['grav'] = flows_df['dist decay'] * flows_df['att']\n",
    "            peformance_mx['flowhdbscan_{}_{}_{}'.format(mf,mc,ms)] = flows_df.groupby('oa_id').sum()['grav']\n",
    "            costing_mx['flowhdbscan_{}_{}_{}'.format(mf,mc,ms)] = flows_df.groupby('oa_id').sum()['queryTime']\n",
    "            computing_times['flowhdbscan_{}_{}_{}'.format(mf,mc,ms)] = processing_time\n",
    "            computing_times['get_flow_flowhdbscan_{}_{}_{}'.format(mf,mc,ms)] = get_flows_time\n",
    "\n",
    "\n",
    "print('process gravity trip generator')\n",
    "\n",
    "# Gravit Trip Generator\n",
    "distMxList = []\n",
    "\n",
    "for i,r in oaSample.iterrows():\n",
    "    for i_, r_ in POISample.iterrows():\n",
    "        rowAppend = {}\n",
    "        rowAppend['oa'] = r['oa_id']\n",
    "        rowAppend['poi'] = r_['poi_id']\n",
    "        rowAppend['dist'] = haversine_distance(r['oa_lon'], r['oa_lat'], r_['poi_lon'], r_['poi_lat'])\n",
    "        distMxList.append(rowAppend)\n",
    "\n",
    "distMx = pd.DataFrame(distMxList)\n",
    "distMx['att'] = distMx['poi'].map(attractivnessDict)\n",
    "\n",
    "distsDecay = []\n",
    "for i in np.array(distMx['dist']):\n",
    "    distsDecay.append(distance_decay(i, decay_constant, exponent))\n",
    "\n",
    "distMx['decay'] = distsDecay\n",
    "distMx['grav'] = distMx['decay'] * distMx['att']\n",
    "distMx['gravN'] = (distMx['grav'] - distMx['grav'].min()) / (distMx['grav'].max() - distMx['grav'].min())\n",
    "\n",
    "distMx = distMx.merge(trips.groupby(['oa_id','poi_id']).count()['departure_time'].rename('tripCount'), left_on = ['oa','poi'],right_index = True)\n",
    "distMx['tripsSample'] = distMx['tripCount'] * distMx['gravN']\n",
    "\n",
    "resultsList = []\n",
    "countOrigins = 0\n",
    "\n",
    "for o in list(oaSample['oa_id']):\n",
    "    countOrigins += 1\n",
    "    oCosts = []\n",
    "    oTrips = 0\n",
    "    oTimes = 0\n",
    "    for p in list(POISample['poi_id']):\n",
    "        if len(oa_poi_id_dict[o][p]) > 0:\n",
    "            numSample = int(distMx[(distMx['oa'] == o) & (distMx['poi'] == p)]['tripsSample'].values[0])\n",
    "            tripSample = trips.loc[oa_poi_id_dict[o][p]].sample(numSample)\n",
    "            oCosts = oCosts + list(tripSample['gac'])\n",
    "            oTrips += numSample\n",
    "            oTimes += tripSample['queryTime'].sum()\n",
    "    rowAppend = {}\n",
    "    rowAppend['OA'] = o\n",
    "    rowAppend['GTG'] = statistics.mean(oCosts)\n",
    "    rowAppend['GTG_Trips'] = oTrips\n",
    "    rowAppend['GTG_Times'] = oTimes\n",
    "    resultsList.append(rowAppend)\n",
    "\n",
    "tripGenGrav = pd.DataFrame(resultsList).set_index('OA')\n",
    "\n",
    "peformance_mx['g-tgm'] = tripGenGrav['GTG']\n",
    "costing_mx['g-tgm'] = tripGenGrav['GTG_Times']\n",
    "\n",
    "performance[it][stratum] = peformance_mx\n",
    "processing_times[it][stratum] = costing_mx\n",
    "exp_meta_data[it][stratum]['processing_times'] = computing_times\n",
    "\n",
    "with open('results/performance.pkl', 'wb') as f:\n",
    "    pickle.dump(performance, f)\n",
    "\n",
    "with open('results/processing_times.pkl', 'wb') as f:\n",
    "    pickle.dump(processing_times, f)\n",
    "\n",
    "with open('results/exp_meta_data.pkl', 'wb') as f:\n",
    "    pickle.dump(exp_meta_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "access",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
